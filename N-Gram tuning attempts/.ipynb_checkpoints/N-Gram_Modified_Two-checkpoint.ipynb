{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5859db",
   "metadata": {},
   "source": [
    "# Implementation of N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629e0a0",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a5fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 250\n",
    "Batch_Size = 64\n",
    "epochs = 5\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e86ce",
   "metadata": {},
   "source": [
    "Creating N-Gram Vectors set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26fd728e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.214.24.120/C/2.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valleyobgynassociates.com/wp-content/vp3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112.248.152.255:57033/Mozi.m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classmates.com/directory/public/memberprofile/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moneytalks.com/main.htm?id=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781327</th>\n",
       "      <td>paypal.com.secure.information.vineyardadvice.c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781328</th>\n",
       "      <td>local.yahoo.com/info-18276551-st-paul-s-episco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781329</th>\n",
       "      <td>nps.gov/fone/historyculture/index.htm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781330</th>\n",
       "      <td>culinaryinstitutemi.com/faculty/jamie-leroux/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781331</th>\n",
       "      <td>nj.com/entertainment/tv/index.ssf/2011/09/char...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>781332 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  type\n",
       "0                                   95.214.24.120/C/2.txt     0\n",
       "1                valleyobgynassociates.com/wp-content/vp3     0\n",
       "2                            112.248.152.255:57033/Mozi.m     0\n",
       "3       classmates.com/directory/public/memberprofile/...     1\n",
       "4                             moneytalks.com/main.htm?id=     1\n",
       "...                                                   ...   ...\n",
       "781327  paypal.com.secure.information.vineyardadvice.c...     0\n",
       "781328  local.yahoo.com/info-18276551-st-paul-s-episco...     1\n",
       "781329              nps.gov/fone/historyculture/index.htm     1\n",
       "781330      culinaryinstitutemi.com/faculty/jamie-leroux/     1\n",
       "781331  nj.com/entertainment/tv/index.ssf/2011/09/char...     1\n",
       "\n",
       "[781332 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\"https://\",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\"http://\",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\" \",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\"www\",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.strip()\n",
    "url_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9f877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_path(link):\n",
    "    return re.sub(r\"/.*\", \"\", link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60aaf6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.214.24.120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valleyobgynassociates.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112.248.152.255:57033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classmates.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moneytalks.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781327</th>\n",
       "      <td>paypal.com.secure.information.vineyardadvice.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781328</th>\n",
       "      <td>local.yahoo.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781329</th>\n",
       "      <td>nps.gov</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781330</th>\n",
       "      <td>culinaryinstitutemi.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781331</th>\n",
       "      <td>nj.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>781332 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  type\n",
       "0                                          95.214.24.120     0\n",
       "1                              valleyobgynassociates.com     0\n",
       "2                                  112.248.152.255:57033     0\n",
       "3                                         classmates.com     1\n",
       "4                                         moneytalks.com     1\n",
       "...                                                  ...   ...\n",
       "781327  paypal.com.secure.information.vineyardadvice.com     0\n",
       "781328                                   local.yahoo.com     1\n",
       "781329                                           nps.gov     1\n",
       "781330                           culinaryinstitutemi.com     1\n",
       "781331                                            nj.com     1\n",
       "\n",
       "[781332 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_dataframe['url'] = url_dataframe['url'].apply(remove_path)\n",
    "url_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22194157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      781332.000000\n",
       "mean           17.046361\n",
       "std            10.398464\n",
       "min             0.000000\n",
       "25%            12.000000\n",
       "50%            16.000000\n",
       "75%            20.000000\n",
       "90%            24.000000\n",
       "98%            35.000000\n",
       "99%            45.000000\n",
       "99.99%        224.866900\n",
       "99.999%       239.373380\n",
       "100%          319.000000\n",
       "max           319.000000\n",
       "Name: url, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = url_dataframe['url'].str.len()\n",
    "x.describe(percentiles=[0.25,0.5,0.75,0.9,0.98,0.99,0.9999,0.99999,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01a1fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\x0eeira¯&nvinip¿ncH¯wVö%ÆåyDaHðû\\nÓ\\x176(rTÃ\\x06u\\x0f\\x8f\\x7fæ\\x82\\x0c\\x99=g\\x810¾\\x96÷mÖi\\x12Ó-;\\x9bXZ\\\\%êýü\\x05Éfn&\\x87\\\\°%7õÙ:¹u\\x96\\x0f\\x161ÌÑêFÄòW<\\x18\\x80$cï\\x86¦t[\\x910ò\\x9f>Þj\\x93®ÆeV2\\x92à\\x1bpù-íàÇ$E¤ZëÈ²ú\\x16SÌ¶\\tp\\x1fáSò°i°vþ[«³»]¹\\rjlÛW¿\\x95\\x9b\\x8a]ø¾µj¿;\\x08·ªo!\\x94ÒPì\\xa0·Ê\\x8bïH§#'\\x823\\x1bø@CÄR\\x02õ²çÇ\\x17\\x17Ý®\\x16ö\\x1d\\x7fQBÇÆg`Èå\\x85Zéê\\xa0D\\x1cîÂm\\x9e®ÎÝQó*x;9?\\x0fÁ\\x19\\x81Òâ\\x88\\x99bùt\\x1bÖ\\x07\\x96Ù®mÞ\\x80N\\x94\\x97P¯°^M\\x8eQ\\n(\\x1f-\\x04\\x06§;¬ÔAèUè\\x99é\\\\\\x10¨ø\\x95íÌ\\x88dB\\\\\\x01\\x8b\\x12[q½=ÿVuÃ\\x01»\\x07râæH\\x1bä\\x8dô\",\n",
       " '¨R\\x98ÊÃ\\x86ûaCóÞit×ßÂe-DÖ\\x8bØ+9YèÌçÏ\\x97¯·\\x04\"0£ÙÕ.0ößF«7¹N\\x89R\\x1c\\x04Ù{ccÉÄãéçx[Ä6a\\x1a5Ñ³LÖíÜÉÀ£\\x9dÒma¥yRX\\x03\\x9a*0ÅÝ7×Ê\\x83ÁÌ\\x05\\x05o«Õs¶\\x8d0k\\x90dèÑ&\\x83\\x1cÄ\\x10\"Ï¨mZ\\'àD\\x8fM×ñ\\x01XÚÒK\"päî±h¬\\x83cAÊeK@4r\"^\\'ÓFþ1*Ë\\x8e\\x1d\\x8dËPÞô;õ\\x9c$úàÑ@\\x87þ=êWÑ\"Ãhñ\\x05\\x18®ç^\\x18\\x11«Ýó^ç\\x1a\\x1fRú\\x8eUJ\\x14.<6C\\x19\\x94y\\x1a\\x9fÜ\\x94FØrÿV2ôæý\\x89\\x03Zãii\\x16\\x93I\\x0e\\x8ab;\\x13\\x16¨Ë\\x97\\x9cµu^Í\\x99V\\x90y)\\x9d\\xadè»âýº\\x01+\\x9f\\x94S\\x99Ö\\x1e\\x17á\\x10\\x95\\x03Ãì?\\x1få6åÔ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_links = [link for link in url_dataframe['url'] if len(link) > 250]\n",
    "long_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f941c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 01:01:52.596401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 01:01:52.982532: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-21 01:01:52.982557: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-21 01:01:53.029470: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-21 01:01:53.975966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 01:01:53.976045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 01:01:53.976052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91391e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 01:01:55.500080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-21 01:01:55.500402: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-21 01:01:55.500436: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (munna0912-VivoBook-ASUSLaptop-X421IAY-M413IA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-21 01:01:55.503475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6162bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e5cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "350c81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   output_sequence_length=Sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0eab898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ef2619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38927"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = len(Vectorize_Layer.get_vocabulary())\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1690f25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'c o m',\n",
       " 's c o',\n",
       " 'e c o',\n",
       " 'o r g',\n",
       " 'n e t',\n",
       " 'n c o',\n",
       " 't c o',\n",
       " 'a c o',\n",
       " 'y c o',\n",
       " 'e d i',\n",
       " 'o c o',\n",
       " 'r c o',\n",
       " 'l c o',\n",
       " 'i n g',\n",
       " 'd i a',\n",
       " 'e s c',\n",
       " 'i o n',\n",
       " 'l o g',\n",
       " 'e n t',\n",
       " 'l i n',\n",
       " 'e r s',\n",
       " 'i k i',\n",
       " 'w i k',\n",
       " 't e r',\n",
       " 'e r c',\n",
       " 'k c o',\n",
       " 'i n e',\n",
       " 't o r',\n",
       " 's p o',\n",
       " 't h e',\n",
       " 'a o r',\n",
       " 'd c o',\n",
       " 't i o',\n",
       " 'c o u',\n",
       " 'p e d',\n",
       " 'o n c',\n",
       " 'e s t',\n",
       " 'w e b',\n",
       " 'a n d',\n",
       " 'i a o',\n",
       " 'f o r',\n",
       " 'a c e',\n",
       " 'n w i',\n",
       " 'e n w',\n",
       " 'i p e',\n",
       " '1 1 7',\n",
       " 'e n e',\n",
       " 's t e',\n",
       " 'k i p',\n",
       " 'b l o',\n",
       " 'o u t',\n",
       " 'e d u',\n",
       " 'a t i',\n",
       " 'a l l',\n",
       " 's t a',\n",
       " 'c c o',\n",
       " 'l e c',\n",
       " 'a l c',\n",
       " 'a t e',\n",
       " 'o r t',\n",
       " 'p o r',\n",
       " 't r a',\n",
       " 's t r',\n",
       " 'o m b',\n",
       " 'r e s',\n",
       " 'a s t',\n",
       " 'a n c',\n",
       " 'i n c',\n",
       " 'o o k',\n",
       " 'i c o',\n",
       " 'r e c',\n",
       " 'm b r',\n",
       " 'o u k',\n",
       " 'b o o',\n",
       " 'g c o',\n",
       " 'a r t',\n",
       " 'o m a',\n",
       " 'i v e',\n",
       " 'a g e',\n",
       " 'y o u',\n",
       " 'i s t',\n",
       " 'e b o',\n",
       " 'e s s',\n",
       " 'v e r',\n",
       " 'r i c',\n",
       " 't e c',\n",
       " 'b c o',\n",
       " 'g e n',\n",
       " 'e r i',\n",
       " 'p c o',\n",
       " 'r s c',\n",
       " 'o g s',\n",
       " 'b e c',\n",
       " 'f a c',\n",
       " 's t o',\n",
       " 'r e a',\n",
       " 'n t e',\n",
       " '1 7 2',\n",
       " 'r e e',\n",
       " 'o n e',\n",
       " 'c o n',\n",
       " 'l a n',\n",
       " 'i a c',\n",
       " 'u b e',\n",
       " 'n c e',\n",
       " 'e a l',\n",
       " 'a n t',\n",
       " 'a r c',\n",
       " 'n f o',\n",
       " 'm a r',\n",
       " 'o t c',\n",
       " 'p o t',\n",
       " 'c e b',\n",
       " 'o k c',\n",
       " 'i t e',\n",
       " 't u b',\n",
       " 'a n s',\n",
       " 'p r o',\n",
       " 'a m a',\n",
       " 'u t u',\n",
       " 'i c a',\n",
       " 'c h e',\n",
       " 't i n',\n",
       " 'm c o',\n",
       " 'm a n',\n",
       " 'o n s',\n",
       " 'o u r',\n",
       " 's e c',\n",
       " 'c e c',\n",
       " 'g s p',\n",
       " 'w o r',\n",
       " 'h o o',\n",
       " 'e b a',\n",
       " 'e n c',\n",
       " 'r a n',\n",
       " 'i d e',\n",
       " 'n e s',\n",
       " 'i n t',\n",
       " 'i n d',\n",
       " 'r e n',\n",
       " 'p l e',\n",
       " 's o r',\n",
       " 'i n f',\n",
       " 's e r',\n",
       " 'a m e',\n",
       " '1 2 3',\n",
       " 'o m e',\n",
       " 't s c',\n",
       " 'o n t',\n",
       " 'n g c',\n",
       " 't r e',\n",
       " 'd i n',\n",
       " 't a l',\n",
       " 'i l l',\n",
       " 'n e w',\n",
       " 'p e r',\n",
       " 'i n k',\n",
       " 'i a n',\n",
       " 'n e c',\n",
       " 't i c',\n",
       " 'g i n',\n",
       " 's p a',\n",
       " '1 1 5',\n",
       " 's w e',\n",
       " 'o n l',\n",
       " 'm e s',\n",
       " 'a s s',\n",
       " 'a n a',\n",
       " 'a r e',\n",
       " 'r y c',\n",
       " 'c t i',\n",
       " 'a p p',\n",
       " 'o s t',\n",
       " 'c o r',\n",
       " 't e s',\n",
       " 'l l e',\n",
       " 'b e r',\n",
       " 'h c o',\n",
       " 'n l i',\n",
       " 'l e s',\n",
       " 'n d e',\n",
       " 'a r i',\n",
       " 'a r d',\n",
       " 'p a l',\n",
       " 'b a n',\n",
       " 'c e s',\n",
       " 'n t a',\n",
       " 'e c t',\n",
       " 'p r e',\n",
       " 'a h o',\n",
       " '2 1 1',\n",
       " 'z o n',\n",
       " 'h e r',\n",
       " 'r c h',\n",
       " 'u s i',\n",
       " 'e c h',\n",
       " 'c h i',\n",
       " '2 2 2',\n",
       " 'm u s',\n",
       " '2 2 1',\n",
       " 't r o',\n",
       " 'a s e',\n",
       " 'f r e',\n",
       " '1 8 2',\n",
       " 'o n a',\n",
       " 'i c s',\n",
       " 'a n e',\n",
       " 'l a s',\n",
       " 'e l l',\n",
       " 'p a y',\n",
       " 'n t r',\n",
       " 'b a l',\n",
       " 's i n',\n",
       " 'c h a',\n",
       " 'u r e',\n",
       " 'i r e',\n",
       " 'n t c',\n",
       " 'c a r',\n",
       " 'i t y',\n",
       " 'r i n',\n",
       " 'm o n',\n",
       " 'c a n',\n",
       " 'o r d',\n",
       " 'o o l',\n",
       " 'a n g',\n",
       " 'y p a',\n",
       " 'r t s',\n",
       " 'r o n',\n",
       " 's n e',\n",
       " 'm i t',\n",
       " 'g r a',\n",
       " '2 1 2',\n",
       " 'o o c',\n",
       " 't r i',\n",
       " 's o n',\n",
       " 'e r e',\n",
       " 'e a r',\n",
       " 'a d e',\n",
       " 'i c e',\n",
       " 'o r e',\n",
       " 's i c',\n",
       " 'm a u',\n",
       " 'o p l',\n",
       " 'g r o',\n",
       " 's t i',\n",
       " 'a l i',\n",
       " 'b i n',\n",
       " 'e w s',\n",
       " 'a l o',\n",
       " 'i t a',\n",
       " 'y a h',\n",
       " 'a y p',\n",
       " 'a l e',\n",
       " '4 2 2',\n",
       " 'n s c',\n",
       " '8 2 1',\n",
       " '1 7 1',\n",
       " 'o u n',\n",
       " 'd e r',\n",
       " 't t e',\n",
       " 'r a c',\n",
       " 'p l a',\n",
       " 'i m e',\n",
       " 's i t',\n",
       " 'n n e',\n",
       " 'r a t',\n",
       " 'n k e',\n",
       " 'e l e',\n",
       " 'm e d',\n",
       " 't a c',\n",
       " 'c a l',\n",
       " 'i e s',\n",
       " 'c e n',\n",
       " 'd i s',\n",
       " 'm e r',\n",
       " 'h o m',\n",
       " 'w e r',\n",
       " 'e o r',\n",
       " 'i n s',\n",
       " '7 2 1',\n",
       " 'a z o',\n",
       " '2 2 3',\n",
       " 'm a z',\n",
       " 'p a r',\n",
       " 'o g i',\n",
       " 'a c h',\n",
       " 'm e n',\n",
       " 's t c',\n",
       " 'p a c',\n",
       " 'e l i',\n",
       " 'r o u',\n",
       " 'k e d',\n",
       " 'b a s',\n",
       " 'c k e',\n",
       " 's c h',\n",
       " 'd e s',\n",
       " 'o r c',\n",
       " 'm a t',\n",
       " 'n g e',\n",
       " 'n d i',\n",
       " 'e r a',\n",
       " 's s c',\n",
       " 'e n d',\n",
       " 'g l e',\n",
       " 'o c k',\n",
       " 'l e g',\n",
       " 'm i n',\n",
       " 's c a',\n",
       " 'h a r',\n",
       " 'o t o',\n",
       " 'e r t',\n",
       " 't a r',\n",
       " 's h o',\n",
       " '1 2 1',\n",
       " 't u r',\n",
       " 'r a d',\n",
       " 'a i n',\n",
       " 'v i e',\n",
       " 'i s c',\n",
       " 'a l a',\n",
       " 'p a g',\n",
       " 'n e r',\n",
       " 'i n a',\n",
       " 'e r o',\n",
       " 'o u s',\n",
       " 'g o o',\n",
       " 'g e s',\n",
       " 't o n',\n",
       " 'e t c',\n",
       " 'x c o',\n",
       " 'd e c',\n",
       " 'u s t',\n",
       " 'i f e',\n",
       " 't i m',\n",
       " 's o l',\n",
       " 'u s e',\n",
       " 'e r n',\n",
       " 's s i',\n",
       " 'r n e',\n",
       " 'l i f',\n",
       " 't e n',\n",
       " 'o l s',\n",
       " '1 5 5',\n",
       " '1 1 2',\n",
       " 't e d',\n",
       " 'a v e',\n",
       " '1 2 5',\n",
       " 'i c k',\n",
       " 's a n',\n",
       " 'h o t',\n",
       " 'a r y',\n",
       " 'o p e',\n",
       " 'a r k',\n",
       " 'f e c',\n",
       " 'e n s',\n",
       " 'e r v',\n",
       " 'e i n',\n",
       " 'a t o',\n",
       " 'e c a',\n",
       " 'c h c',\n",
       " 'r i s',\n",
       " '1 2 2',\n",
       " 'a d i',\n",
       " '2 3 1',\n",
       " 'u n i',\n",
       " 'o t e',\n",
       " 'i c c',\n",
       " 'n o r',\n",
       " 'a i l',\n",
       " 'u n t',\n",
       " 'i e t',\n",
       " 'f i l',\n",
       " 'n i c',\n",
       " '2 1 6',\n",
       " 'c o l',\n",
       " 't e a',\n",
       " 'o u p',\n",
       " 'i t s',\n",
       " 'n d a',\n",
       " 't e l',\n",
       " 'd e n',\n",
       " 'l i c',\n",
       " 'o r i',\n",
       " 'n t i',\n",
       " 'r i t',\n",
       " 'e m a',\n",
       " 'a c t',\n",
       " 'l a c',\n",
       " 'r o c',\n",
       " 'a l s',\n",
       " 'd a t',\n",
       " 'h o s',\n",
       " 'l e n',\n",
       " 'h e n',\n",
       " 'n a t',\n",
       " 'c i t',\n",
       " 'c h o',\n",
       " 'a n i',\n",
       " 'o n o',\n",
       " 'n t o',\n",
       " 's o u',\n",
       " 'u c o',\n",
       " 'n a l',\n",
       " 'o o t',\n",
       " 'e s i',\n",
       " 'r i a',\n",
       " 't o o',\n",
       " 't c h',\n",
       " 'e d e',\n",
       " 'd b c',\n",
       " 'o g l',\n",
       " 'c a t',\n",
       " 'o o g',\n",
       " 'l l a',\n",
       " 'c a c',\n",
       " 'l i s',\n",
       " 'r u m',\n",
       " 'n s w',\n",
       " 'o m c',\n",
       " 'k i n',\n",
       " 'v e l',\n",
       " 'm i c',\n",
       " 'l s c',\n",
       " 'b r a',\n",
       " 'o l u',\n",
       " 'e b s',\n",
       " 'a r a',\n",
       " 'l o c',\n",
       " 'o r a',\n",
       " 'a t c',\n",
       " 'a s c',\n",
       " 'g o v',\n",
       " 'n i n',\n",
       " 'f i n',\n",
       " 'e a t',\n",
       " 'a c c',\n",
       " 'n e a',\n",
       " 'a m i',\n",
       " 'l e a',\n",
       " 'v e n',\n",
       " 'i l e',\n",
       " 'a c k',\n",
       " 'p a s',\n",
       " 'o s c',\n",
       " 'e a s',\n",
       " 'n g o',\n",
       " 't a n',\n",
       " 'b s c',\n",
       " 'u r c',\n",
       " 'c u r',\n",
       " '1 9 1',\n",
       " 'o r u',\n",
       " 'e p a',\n",
       " 's h a',\n",
       " 'e s p',\n",
       " 'e o p',\n",
       " 'o l l',\n",
       " 'r r e',\n",
       " 's c r',\n",
       " 'l e r',\n",
       " 'm e t',\n",
       " 'a c a',\n",
       " '2 5 4',\n",
       " 't n e',\n",
       " 'r i e',\n",
       " '1 1 3',\n",
       " 'h i n',\n",
       " 'e n g',\n",
       " 'n a r',\n",
       " 'e s o',\n",
       " 'k e r',\n",
       " 'u n d',\n",
       " 's i e',\n",
       " 'n d c',\n",
       " '7 1 9',\n",
       " 's o f',\n",
       " '1 1 4',\n",
       " 'w c o',\n",
       " 'g o c',\n",
       " 'r e d',\n",
       " 's h i',\n",
       " 'd s c',\n",
       " 'c i a',\n",
       " 'r o o',\n",
       " 'r o s',\n",
       " 'a l t',\n",
       " 'e v e',\n",
       " 'r t c',\n",
       " 'o v i',\n",
       " 'm y s',\n",
       " 'u t e',\n",
       " 's e n',\n",
       " 'e t r',\n",
       " 'g e r',\n",
       " 'e t f',\n",
       " 'p r i',\n",
       " 'k e t',\n",
       " 'p p l',\n",
       " 'p e o',\n",
       " 'o m m',\n",
       " 't o p',\n",
       " 's e a',\n",
       " 'l s i',\n",
       " 'l u t',\n",
       " 't f o',\n",
       " '1 1 1',\n",
       " 'e s e',\n",
       " 't a t',\n",
       " 'l i v',\n",
       " 'l o r',\n",
       " 'c g i',\n",
       " 'm i l',\n",
       " 'o r l',\n",
       " 's g o',\n",
       " 'h a n',\n",
       " 'n g s',\n",
       " 'h e a',\n",
       " 'i m a',\n",
       " 'r a i',\n",
       " 'c a s',\n",
       " 'd i c',\n",
       " 'a s i',\n",
       " '2 2 4',\n",
       " 'o v e',\n",
       " 'i m d',\n",
       " 'r t i',\n",
       " 'i n i',\n",
       " 'm a s',\n",
       " 'r m a',\n",
       " 'o o d',\n",
       " 'a b o',\n",
       " 'u l t',\n",
       " 'r a l',\n",
       " 'f i r',\n",
       " 'r d e',\n",
       " 'o c a',\n",
       " 'i t t',\n",
       " '1 5 2',\n",
       " 'c r e',\n",
       " 'l l c',\n",
       " 'u n e',\n",
       " 'i g n',\n",
       " 'o k e',\n",
       " 'd u c',\n",
       " 'a y c',\n",
       " 'm a c',\n",
       " 'o d e',\n",
       " 'v i c',\n",
       " 'l l i',\n",
       " 'y l i',\n",
       " 'e c u',\n",
       " 'd i o',\n",
       " 'r c e',\n",
       " 'i t i',\n",
       " 'm d b',\n",
       " 'v e c',\n",
       " 'b r i',\n",
       " 'a n k',\n",
       " 'i g h',\n",
       " '1 1 9',\n",
       " 'f a m',\n",
       " 'o l i',\n",
       " 'r i v',\n",
       " 'm o v',\n",
       " 'e d c',\n",
       " 's s e',\n",
       " 'e x t',\n",
       " 'h o c',\n",
       " 'l i t',\n",
       " 't y c',\n",
       " 'e t i',\n",
       " 'a t t',\n",
       " 'e g r',\n",
       " 'u m i',\n",
       " 'o r n',\n",
       " 'o g y',\n",
       " 'a t h',\n",
       " 'i s h',\n",
       " '1 4 2',\n",
       " 'f a n',\n",
       " 'l a r',\n",
       " 't s u',\n",
       " '2 1 3',\n",
       " 'i c t',\n",
       " 'i b i',\n",
       " 's u i',\n",
       " 'y s p',\n",
       " 't r y',\n",
       " 'a s h',\n",
       " '1 1 6',\n",
       " 'r e p',\n",
       " 'o m p',\n",
       " 'a d a',\n",
       " 'h o p',\n",
       " 'm y l',\n",
       " 'e e d',\n",
       " 'o n d',\n",
       " 'v i s',\n",
       " 'i l i',\n",
       " 'c l e',\n",
       " 'a p e',\n",
       " 'b l e',\n",
       " 'm o r',\n",
       " 'c s c',\n",
       " 'm i x',\n",
       " 'b i l',\n",
       " 'o r s',\n",
       " 'r v i',\n",
       " 'm o t',\n",
       " 'n s u',\n",
       " 'e m o',\n",
       " '2 1 5',\n",
       " 'e n i',\n",
       " 'u s a',\n",
       " 's l a',\n",
       " 'v a l',\n",
       " '2 1 4',\n",
       " 'o f t',\n",
       " 'o i n',\n",
       " 'n a d',\n",
       " 'n w e',\n",
       " 's t u',\n",
       " 's a l',\n",
       " 'i e n',\n",
       " '1 6 3',\n",
       " 'o m i',\n",
       " 'n e d',\n",
       " 'r l d',\n",
       " 'n i t',\n",
       " 'z c o',\n",
       " 'n s t',\n",
       " 'i a l',\n",
       " 'i o c',\n",
       " 's i g',\n",
       " 'a u t',\n",
       " 'e l c',\n",
       " 'e x p',\n",
       " 's a c',\n",
       " 's y a',\n",
       " 'm e c',\n",
       " 't o c',\n",
       " 'e t s',\n",
       " 's m a',\n",
       " 'e s g',\n",
       " '2 7 4',\n",
       " 'e a c',\n",
       " 'a n n',\n",
       " '1 2 4',\n",
       " '2 1 9',\n",
       " 'e r g',\n",
       " 'o n i',\n",
       " 'c y c',\n",
       " 'o t s',\n",
       " 's o c',\n",
       " '1 0 1',\n",
       " 'r g e',\n",
       " 'r a v',\n",
       " 'p o s',\n",
       " 'g u i',\n",
       " 'i l y',\n",
       " 'e t a',\n",
       " 'w s c',\n",
       " 'o r r',\n",
       " 't h o',\n",
       " 'g a m',\n",
       " 'g e c',\n",
       " 'a v i',\n",
       " 'e p o',\n",
       " 'l e t',\n",
       " 'l d e',\n",
       " '1 5 4',\n",
       " 't r u',\n",
       " 'p h o',\n",
       " 'l l o',\n",
       " 'e m e',\n",
       " 'l i b',\n",
       " 'e v i',\n",
       " 'o t t',\n",
       " 'r a p',\n",
       " 'p a t',\n",
       " 'u k u',\n",
       " '1 6 1',\n",
       " 'e s a',\n",
       " 'e r r',\n",
       " '5 9 9',\n",
       " 'u s c',\n",
       " 'k u m',\n",
       " 'e l a',\n",
       " '1 4 1',\n",
       " '3 1 1',\n",
       " 'g i b',\n",
       " '5 2 1',\n",
       " 'm a g',\n",
       " 'y r i',\n",
       " '1 2 0',\n",
       " 'd o c',\n",
       " 'a t a',\n",
       " 'y u k',\n",
       " 't i v',\n",
       " 'f e r',\n",
       " 'a b a',\n",
       " 'v i d',\n",
       " 'u t o',\n",
       " 'c l a',\n",
       " 'l o n',\n",
       " 'i d a',\n",
       " 'r i o',\n",
       " 't o m',\n",
       " 'w a r',\n",
       " 'o n g',\n",
       " 'r t e',\n",
       " 'o r k',\n",
       " 'v c o',\n",
       " 'u r a',\n",
       " '2 1 7',\n",
       " '2 4 1',\n",
       " 'l y r',\n",
       " 's u p',\n",
       " 'h o r',\n",
       " 'e t t',\n",
       " 'e a d',\n",
       " 'n d o',\n",
       " 'l n e',\n",
       " 'o l o',\n",
       " 't w i',\n",
       " 'p s c',\n",
       " 'e b i',\n",
       " '2 0 1',\n",
       " 'u c k',\n",
       " 'r d s',\n",
       " '4 1 1',\n",
       " 'e r d',\n",
       " 'a b s',\n",
       " 'r n a',\n",
       " '1 5 1',\n",
       " 't l e',\n",
       " 'e g e',\n",
       " 'd a n',\n",
       " '3 4 5',\n",
       " 'i x h',\n",
       " 'd r i',\n",
       " 'x h j',\n",
       " 'o n n',\n",
       " 'i j y',\n",
       " 'h j p',\n",
       " 'c l o',\n",
       " '1 1 8',\n",
       " 'a r s',\n",
       " 'j y u',\n",
       " 'u i j',\n",
       " 'r i p',\n",
       " '1 9 4',\n",
       " 's s o',\n",
       " 'k e y',\n",
       " 'i s e',\n",
       " 'n a n',\n",
       " 'r u n',\n",
       " 'r t h',\n",
       " 'e r b',\n",
       " 'e n a',\n",
       " 'e a m',\n",
       " 'a i r',\n",
       " 'u r i',\n",
       " 'i l m',\n",
       " 'e b c',\n",
       " '1 3 1',\n",
       " 'b o u',\n",
       " 'o d c',\n",
       " '2 3 4',\n",
       " 'r s t',\n",
       " 'o w n',\n",
       " 'i c h',\n",
       " 'l a t',\n",
       " 'o r y',\n",
       " 'h e l',\n",
       " 'p e n',\n",
       " 'd e l',\n",
       " 'q u e',\n",
       " 'r c a',\n",
       " 'e a n',\n",
       " 'n a c',\n",
       " 'n t s',\n",
       " 'e b e',\n",
       " 'e f o',\n",
       " 'e s n',\n",
       " '4 5 1',\n",
       " 'l c a',\n",
       " '1 6 2',\n",
       " 'u p s',\n",
       " 'i e r',\n",
       " 'e l p',\n",
       " 'c a p',\n",
       " 'v e s',\n",
       " 'd o r',\n",
       " 'e g a',\n",
       " 'e r m',\n",
       " 'b a r',\n",
       " 'a c i',\n",
       " '4 1 2',\n",
       " 'o m o',\n",
       " 'r e i',\n",
       " 'g h t',\n",
       " 'h o u',\n",
       " 'c r o',\n",
       " 'i n w',\n",
       " 'l i a',\n",
       " 'v i n',\n",
       " 'p o k',\n",
       " 'e l o',\n",
       " 'd r e',\n",
       " 'l u s',\n",
       " '4 2 1',\n",
       " 'y o r',\n",
       " 'k i a',\n",
       " 'g r e',\n",
       " 'd e m',\n",
       " 'm a i',\n",
       " 'w i t',\n",
       " '1 1 0',\n",
       " 'f i c',\n",
       " '2 3 9',\n",
       " 'n d s',\n",
       " 's i a',\n",
       " 'e n n',\n",
       " 'a n o',\n",
       " 'n g l',\n",
       " 'o r m',\n",
       " 'f o o',\n",
       " 'h o l',\n",
       " 'o l a',\n",
       " 'n c h',\n",
       " 'i g i',\n",
       " 'a k e',\n",
       " 'e r p',\n",
       " 'n o m',\n",
       " 'a m o',\n",
       " '2 1 8',\n",
       " '1 5 6',\n",
       " 'n e x',\n",
       " 'e f r',\n",
       " 'c m d',\n",
       " 'u t i',\n",
       " 'i t c',\n",
       " '2 5 2',\n",
       " 'n s o',\n",
       " '6 1 5',\n",
       " 't e m',\n",
       " 'm y c',\n",
       " 'c t o',\n",
       " 'p e c',\n",
       " 'h i v',\n",
       " 'h i s',\n",
       " '5 1 1',\n",
       " '2 3 5',\n",
       " 'e m i',\n",
       " 'i v a',\n",
       " 'n i a',\n",
       " 'r o t',\n",
       " 'e e n',\n",
       " 'r t a',\n",
       " '2 5 1',\n",
       " 'f c o',\n",
       " 'u i d',\n",
       " 'r a r',\n",
       " 't s w',\n",
       " '1 7 5',\n",
       " 'i c i',\n",
       " 'w n c',\n",
       " 's i o',\n",
       " 'r o p',\n",
       " 'd a c',\n",
       " 'c e l',\n",
       " 'p a n',\n",
       " 'p i n',\n",
       " 'b a b',\n",
       " 'r e m',\n",
       " 'b e s',\n",
       " 'd e a',\n",
       " 'n a m',\n",
       " '7 2 0',\n",
       " 'l o w',\n",
       " '1 8 5',\n",
       " '1 9 2',\n",
       " 't b a',\n",
       " 'r a s',\n",
       " 'b s o',\n",
       " 'o l d',\n",
       " 'm a l',\n",
       " '5 4 1',\n",
       " 'e t e',\n",
       " 'r e f',\n",
       " 'e g o',\n",
       " 'e t o',\n",
       " '4 1 5',\n",
       " 'r s e',\n",
       " 'n s i',\n",
       " '1 0 3',\n",
       " 't o u',\n",
       " 's p e',\n",
       " '9 2 1',\n",
       " 'u r n',\n",
       " '0 1 1',\n",
       " '1 8 0',\n",
       " 'e h o',\n",
       " 'm s c',\n",
       " 'r s o',\n",
       " 'd p r',\n",
       " 'c o i',\n",
       " '1 7 9',\n",
       " '1 5 3',\n",
       " 'n e l',\n",
       " '9 4 1',\n",
       " '2 1 0',\n",
       " 'u m c',\n",
       " 'e b l',\n",
       " 'r k e',\n",
       " 'o m u',\n",
       " 'a s a',\n",
       " 's b l',\n",
       " 'c i n',\n",
       " 'c c a',\n",
       " '7 2 2',\n",
       " 'h i l',\n",
       " 'c o z',\n",
       " 'b u s',\n",
       " 'e l s',\n",
       " 'a r o',\n",
       " 'o s e',\n",
       " 'o c c',\n",
       " 'n s a',\n",
       " 'b a y',\n",
       " 'a g o',\n",
       " 'i n o',\n",
       " 'i c l',\n",
       " '4 5 0',\n",
       " '9 1 5',\n",
       " 'd e o',\n",
       " 'a r m',\n",
       " '1 3 8',\n",
       " 'c a m',\n",
       " 'l e b',\n",
       " 'i p l',\n",
       " 'w i n',\n",
       " 'i f y',\n",
       " 'g e l',\n",
       " 'a m p',\n",
       " 'd a y',\n",
       " 'b r o',\n",
       " 't e h',\n",
       " 'c c e',\n",
       " 't o s',\n",
       " 'i o r',\n",
       " 'g y c',\n",
       " 'l e d',\n",
       " 'l o p',\n",
       " 'i b e',\n",
       " 'b o r',\n",
       " 'r o m',\n",
       " 's e b',\n",
       " 'g a r',\n",
       " 's e s',\n",
       " '2 4 8',\n",
       " '2 2 5',\n",
       " 'b l i',\n",
       " 'l l s',\n",
       " 'y c l',\n",
       " 'p i c',\n",
       " 'y n e',\n",
       " 'b e a',\n",
       " '1 3 2',\n",
       " 'l d c',\n",
       " 'l o u',\n",
       " '2 0 8',\n",
       " '3 1 3',\n",
       " '2 5 3',\n",
       " 'l e i',\n",
       " 'e r f',\n",
       " 'c e r',\n",
       " 's i d',\n",
       " 'e d a',\n",
       " 'n e n',\n",
       " '5 2 2',\n",
       " 'e r u',\n",
       " '1 3 5',\n",
       " 'n t u',\n",
       " 'i o s',\n",
       " 'e r y',\n",
       " 'a l p',\n",
       " 'a l m',\n",
       " '2 3 2',\n",
       " '6 1 1',\n",
       " 'l i e',\n",
       " 'b a t',\n",
       " 'u n g',\n",
       " 'h e f',\n",
       " 'b u r',\n",
       " '8 2 5',\n",
       " 't u n',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorize_Layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad60da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed53eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a65900f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(250,), dtype=int64, numpy=\n",
       "array([ 776,  210,  675, 1249, 4276,  949,  859, 1420,   31,  194,   26,\n",
       "         13,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorize_Layer(\"hellobrother.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97135396",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7f1cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 64)           2491328   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 250, 64)           33024     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,561,601\n",
      "Trainable params: 2,561,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "281c1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdfb030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9767/9767 [==============================] - 1866s 191ms/step - loss: 0.6888 - accuracy: 0.5475\n",
      "Epoch 2/5\n",
      "9767/9767 [==============================] - 1819s 186ms/step - loss: 0.6887 - accuracy: 0.5475\n",
      "Epoch 3/5\n",
      "9767/9767 [==============================] - 1798s 184ms/step - loss: 0.6887 - accuracy: 0.5475\n",
      "Epoch 4/5\n",
      "2377/9767 [======>.......................] - ETA: 22:21 - loss: 0.6884 - accuracy: 0.5488"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1727e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bd58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de021990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d01a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 100\n",
    "Batch_Size = 64\n",
    "epochs = 5\n",
    "n_classes = 2\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\"https://\",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\"http://\",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\" \",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.replace(\"www\",\"\")\n",
    "url_dataframe['url'] = url_dataframe['url'].str.strip()\n",
    "print(url_dataframe)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "max_tokens = len(Vectorize_Layer.get_vocabulary())\n",
    "print(\"max_tokens\" = max_tokens)\n",
    "\n",
    "print(Vectorize_Layer.get_vocabulary())\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
