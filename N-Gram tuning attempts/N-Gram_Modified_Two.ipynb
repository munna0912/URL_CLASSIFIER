{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5859db",
   "metadata": {},
   "source": [
    "# Implementation of N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629e0a0",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a5fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "import urllib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 300\n",
    "Batch_Size = 64\n",
    "epochs = 5\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238cd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061c6ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extremetubecomvideobittitblondeslutinglassesfu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>playarprintcomchasehomephppublicenrollidentify...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genforumgenealogycommcgeepage15html</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itunesapplecomgbalbumwhbbonustrackversionid358...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>holdingmaclincleieindexphp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770410</th>\n",
       "      <td>facebookcompagesdrstevebrule31743261441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770411</th>\n",
       "      <td>absoluteastronomycomtopicslistoffamilyrelation...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770412</th>\n",
       "      <td>oceanpacificsclmainhtm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770413</th>\n",
       "      <td>viewpointscomsamuelzarkoffcollectioncultclassi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770414</th>\n",
       "      <td>fischereivereinorgterminkalenderyearlistevents...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>770415 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  type\n",
       "0       extremetubecomvideobittitblondeslutinglassesfu...     1\n",
       "1       playarprintcomchasehomephppublicenrollidentify...     0\n",
       "2                     genforumgenealogycommcgeepage15html     1\n",
       "3       itunesapplecomgbalbumwhbbonustrackversionid358...     1\n",
       "4                              holdingmaclincleieindexphp     0\n",
       "...                                                   ...   ...\n",
       "770410            facebookcompagesdrstevebrule31743261441     1\n",
       "770411  absoluteastronomycomtopicslistoffamilyrelation...     1\n",
       "770412                             oceanpacificsclmainhtm     1\n",
       "770413  viewpointscomsamuelzarkoffcollectioncultclassi...     1\n",
       "770414  fischereivereinorgterminkalenderyearlistevents...     0\n",
       "\n",
       "[770415 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "url_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e86ce",
   "metadata": {},
   "source": [
    "Creating N-Gram Vectors set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22194157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      770415.000000\n",
       "mean           44.570860\n",
       "std            35.836788\n",
       "min             0.000000\n",
       "25%            22.000000\n",
       "50%            34.000000\n",
       "75%            56.000000\n",
       "90%            80.000000\n",
       "98%           145.000000\n",
       "99%           189.000000\n",
       "99.99%        573.000000\n",
       "99.999%      1272.000000\n",
       "100%         2141.000000\n",
       "max          2141.000000\n",
       "Name: url, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = url_dataframe['url'].str.len()\n",
    "x.describe(percentiles=[0.25,0.5,0.75,0.9,0.98,0.99,0.9999,0.99999,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f941c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 00:13:44.555351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 00:13:44.673607: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:13:44.673629: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 00:13:44.695908: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-04 00:13:45.218064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:13:45.218125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:13:45.218131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91391e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 00:13:45.998354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:13:45.998384: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-04 00:13:45.998401: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (munna0912-VivoBook-ASUSLaptop-X421IAY-M413IA): /proc/driver/nvidia/version does not exist\n",
      "2023-03-04 00:13:45.998687: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6162bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e5cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350c81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                    max_tokens = 30000,\n",
    "                                                   output_mode='int',\n",
    "                                                   output_sequence_length=Sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0eab898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ef2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = len(Vectorize_Layer.get_vocabulary())\n",
    "max_tokens = 5000\n",
    "# have to tune max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1690f25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'c o m',\n",
       " 'h t m',\n",
       " 'i n d',\n",
       " 'n d e',\n",
       " 't m l',\n",
       " 'e n t',\n",
       " 'p h p',\n",
       " 'i o n',\n",
       " 'd e x',\n",
       " 's c o',\n",
       " 't i o',\n",
       " 'e x p',\n",
       " 'e c o',\n",
       " 'n c o',\n",
       " 'x p h',\n",
       " 'a r t',\n",
       " 'o m p',\n",
       " 'i n g',\n",
       " 'o n t',\n",
       " 'i t e',\n",
       " 'o r g',\n",
       " 'o n c',\n",
       " 'c o n',\n",
       " 'v i e',\n",
       " 'a g e',\n",
       " 'n t e',\n",
       " 'l o g',\n",
       " 'o m c',\n",
       " 'e r s',\n",
       " 'n e t',\n",
       " 'a t e',\n",
       " 't o r',\n",
       " 'a t i',\n",
       " 't e r',\n",
       " 'o z i',\n",
       " 'i e w',\n",
       " 'm o z',\n",
       " 't i c',\n",
       " 'l i n',\n",
       " 't h e',\n",
       " 'e s t',\n",
       " 'a n d',\n",
       " 't e m',\n",
       " 'r t i',\n",
       " 'e d i',\n",
       " 'z i m',\n",
       " 'm c o',\n",
       " 't e n',\n",
       " 's t a',\n",
       " 'p t i',\n",
       " 'e m i',\n",
       " '2 0 1',\n",
       " 't c o',\n",
       " 'o m a',\n",
       " 'o p t',\n",
       " 'a l l',\n",
       " 'i n e',\n",
       " 'l c o',\n",
       " 'm i n',\n",
       " 'c a t',\n",
       " 'm i d',\n",
       " 'i s t',\n",
       " 'o n e',\n",
       " 'o m i',\n",
       " 'f o r',\n",
       " 'g i n',\n",
       " 'i k i',\n",
       " 'p r o',\n",
       " 'r e s',\n",
       " 'c l e',\n",
       " 'w i k',\n",
       " 'p o p',\n",
       " 'o m e',\n",
       " 's t o',\n",
       " 'i c l',\n",
       " 'm a r',\n",
       " 'e s s',\n",
       " 'h p o',\n",
       " 'a c o',\n",
       " 'e s c',\n",
       " 'p a g',\n",
       " 'n e w',\n",
       " 'd i a',\n",
       " 's t e',\n",
       " 't i n',\n",
       " 'o r t',\n",
       " 'i d 1',\n",
       " 'y c o',\n",
       " 'a m e',\n",
       " 'w a r',\n",
       " 'm e n',\n",
       " 'r e a',\n",
       " 'i n s',\n",
       " 'a m p',\n",
       " 'o m s',\n",
       " 'e n e',\n",
       " 'o g i',\n",
       " 'o c o',\n",
       " 'o u t',\n",
       " 'e r i',\n",
       " 'r c o',\n",
       " 'r i n',\n",
       " 'l a n',\n",
       " 't r a',\n",
       " 'r e c',\n",
       " 'i n t',\n",
       " 'o r y',\n",
       " 'o m m',\n",
       " 'o m b',\n",
       " 'w e b',\n",
       " 'e w s',\n",
       " 'o n s',\n",
       " 's t r',\n",
       " 'l l e',\n",
       " 'c o u',\n",
       " 'p o r',\n",
       " 's i n',\n",
       " 'v e r',\n",
       " 'a r c',\n",
       " 'e c t',\n",
       " 's p o',\n",
       " 'l e s',\n",
       " 'r i c',\n",
       " 'n e n',\n",
       " 'e r c',\n",
       " 't r e',\n",
       " 'e w a',\n",
       " 's o n',\n",
       " 'e a r',\n",
       " 'e i n',\n",
       " 'i v e',\n",
       " 'i d e',\n",
       " 'e i d',\n",
       " 'b i n',\n",
       " 'r a n',\n",
       " 'o m t',\n",
       " 'a i l',\n",
       " 'g e s',\n",
       " 'm p a',\n",
       " 's h o',\n",
       " 'i c a',\n",
       " 'a n c',\n",
       " 'c h a',\n",
       " 'p e r',\n",
       " 'r c h',\n",
       " 'm a n',\n",
       " 'r e n',\n",
       " 'l e i',\n",
       " 't v i',\n",
       " 't e s',\n",
       " 'e a l',\n",
       " 's e r',\n",
       " 'y o u',\n",
       " 'm p o',\n",
       " 'o m w',\n",
       " 'b l o',\n",
       " 'a i n',\n",
       " 'a s t',\n",
       " 'l e c',\n",
       " 't c h',\n",
       " 'i e s',\n",
       " 'o u r',\n",
       " 'p l a',\n",
       " 'i n c',\n",
       " 'g e n',\n",
       " 'i l l',\n",
       " 'a s p',\n",
       " 'c h e',\n",
       " 'b o t',\n",
       " 'a c e',\n",
       " 's t i',\n",
       " 'm o n',\n",
       " 'n t a',\n",
       " 'a r d',\n",
       " 'i r e',\n",
       " 's h t',\n",
       " 'c t i',\n",
       " 'n c e',\n",
       " 'a n s',\n",
       " '0 1 1',\n",
       " 'n t v',\n",
       " 'o m d',\n",
       " 'a n t',\n",
       " 'a n g',\n",
       " 'a t t',\n",
       " 'm p l',\n",
       " 'n t r',\n",
       " 'z i p',\n",
       " 'p l e',\n",
       " 'c h i',\n",
       " 'p r e',\n",
       " 'k c o',\n",
       " 'a t c',\n",
       " 'p o n',\n",
       " 'a n a',\n",
       " 'e l e',\n",
       " '2 0 0',\n",
       " 'i t y',\n",
       " 'o t z',\n",
       " 'e r e',\n",
       " 'o n a',\n",
       " 'm e s',\n",
       " 't z i',\n",
       " 'r e e',\n",
       " 'p c o',\n",
       " 'a r e',\n",
       " 'd e r',\n",
       " 'n e s',\n",
       " 'l i s',\n",
       " 't i d',\n",
       " 's e a',\n",
       " 'e l l',\n",
       " 's e c',\n",
       " 'd c o',\n",
       " 'o o k',\n",
       " 'h e r',\n",
       " 'n i n',\n",
       " 'a c t',\n",
       " 't a r',\n",
       " 'e m a',\n",
       " 's i t',\n",
       " 'l a y',\n",
       " 'a l c',\n",
       " '0 0 0',\n",
       " 'n f o',\n",
       " 'c c o',\n",
       " 's p a',\n",
       " 'e r t',\n",
       " 'l e r',\n",
       " 'o u n',\n",
       " 'b e r',\n",
       " 'p e d',\n",
       " 's c h',\n",
       " 'm e r',\n",
       " 'e g o',\n",
       " 'l e n',\n",
       " 'x h t',\n",
       " 'e t a',\n",
       " 't a l',\n",
       " 'c a n',\n",
       " 'd e s',\n",
       " 'n t s',\n",
       " 'u r e',\n",
       " 'a p p',\n",
       " 'o s t',\n",
       " 'a l i',\n",
       " 'w o r',\n",
       " 'c e s',\n",
       " 'i c e',\n",
       " 'i c s',\n",
       " 'f i l',\n",
       " 'e x h',\n",
       " 'a o r',\n",
       " 'i a n',\n",
       " 'i n f',\n",
       " 'm u s',\n",
       " 'a l e',\n",
       " 'p a r',\n",
       " 'e n c',\n",
       " 'b o o',\n",
       " 'n t i',\n",
       " 'g r a',\n",
       " 'e r a',\n",
       " 'a s s',\n",
       " '1 1 7',\n",
       " 'o m 2',\n",
       " 'i n k',\n",
       " 'g o r',\n",
       " 'a r i',\n",
       " 'd i s',\n",
       " 'c a r',\n",
       " 't l e',\n",
       " 'e s i',\n",
       " 'e v e',\n",
       " 'o r d',\n",
       " 'p r i',\n",
       " 'd i n',\n",
       " 'y p a',\n",
       " 'o m n',\n",
       " 'e d u',\n",
       " 't o p',\n",
       " 'u s i',\n",
       " 'i c o',\n",
       " 'i p e',\n",
       " 'a s e',\n",
       " 'c a l',\n",
       " 't e c',\n",
       " 'm 2 0',\n",
       " 't e g',\n",
       " 'u l t',\n",
       " 'i n a',\n",
       " 'k i p',\n",
       " 'i a o',\n",
       " 'i d 2',\n",
       " 'g h t',\n",
       " 'n a l',\n",
       " 'o r e',\n",
       " 'e n d',\n",
       " 'e c a',\n",
       " 'r i e',\n",
       " 'i t i',\n",
       " 'h a r',\n",
       " 'r r e',\n",
       " 't a t',\n",
       " '1 1 0',\n",
       " 't a c',\n",
       " 'u s e',\n",
       " 'e o p',\n",
       " 'n w i',\n",
       " 'n a m',\n",
       " 'e n w',\n",
       " 'n g e',\n",
       " 'v e n',\n",
       " 'i l e',\n",
       " 'm a i',\n",
       " 'b a l',\n",
       " 'o p l',\n",
       " 'm a t',\n",
       " 't a i',\n",
       " 's s i',\n",
       " 'd u c',\n",
       " 't o n',\n",
       " 'a m a',\n",
       " 'm i t',\n",
       " 't u r',\n",
       " 'e b a',\n",
       " 'r g w',\n",
       " 'e a s',\n",
       " 'f a c',\n",
       " 'n n e',\n",
       " 'p a l',\n",
       " 'q b o',\n",
       " 'n l i',\n",
       " 't r i',\n",
       " 'i m a',\n",
       " 'e s e',\n",
       " 'r e d',\n",
       " 'g w i',\n",
       " 's c a',\n",
       " 'o m o',\n",
       " 'h o o',\n",
       " 'u n t',\n",
       " 'k i n',\n",
       " 'l a s',\n",
       " 'b a n',\n",
       " 'e r v',\n",
       " 'h p q',\n",
       " 'o m l',\n",
       " 'a d e',\n",
       " 'i c k',\n",
       " 'e b o',\n",
       " 'o t o',\n",
       " 'c o r',\n",
       " 'e s h',\n",
       " 'n e r',\n",
       " 'e t t',\n",
       " 'd 9 8',\n",
       " 't r o',\n",
       " 'm b r',\n",
       " 'b e c',\n",
       " 'r a t',\n",
       " 's a n',\n",
       " 'p a y',\n",
       " 'h o m',\n",
       " '0 1 0',\n",
       " 'l e a',\n",
       " 'p i c',\n",
       " 't i t',\n",
       " 'h o t',\n",
       " 'a c h',\n",
       " 'r t s',\n",
       " 'g c o',\n",
       " 'n s h',\n",
       " 'h a n',\n",
       " 'q u e',\n",
       " 'b r i',\n",
       " 't t e',\n",
       " 'e n s',\n",
       " 'o r i',\n",
       " 'd a t',\n",
       " 'r o n',\n",
       " 'c e b',\n",
       " 'p l c',\n",
       " 'r i t',\n",
       " 'e l i',\n",
       " 'a c k',\n",
       " 'a n i',\n",
       " 'e v i',\n",
       " 'u n i',\n",
       " 'e c h',\n",
       " 'o r m',\n",
       " 'e r n',\n",
       " 'm e d',\n",
       " '1 1 1',\n",
       " 'i g h',\n",
       " 'n d i',\n",
       " 's i c',\n",
       " 'i t a',\n",
       " 'c t o',\n",
       " 'f r e',\n",
       " 'e a t',\n",
       " 'v i d',\n",
       " 'a y p',\n",
       " 'o p e',\n",
       " 'o n l',\n",
       " 'p q b',\n",
       " 'r o d',\n",
       " 'd i r',\n",
       " 't e d',\n",
       " 'o o l',\n",
       " 'r a c',\n",
       " 's c r',\n",
       " 'd e n',\n",
       " 'w a t',\n",
       " 'o a d',\n",
       " 'l l i',\n",
       " 'f e r',\n",
       " 'e m e',\n",
       " 'n d a',\n",
       " 'c o l',\n",
       " 'a d i',\n",
       " 'r s c',\n",
       " 'a n e',\n",
       " 'c h o',\n",
       " 'i d 5',\n",
       " 'o m r',\n",
       " 'e s p',\n",
       " 'e h t',\n",
       " 'e s a',\n",
       " 'i n i',\n",
       " 'u s t',\n",
       " 't e a',\n",
       " 'm a g',\n",
       " 'e t i',\n",
       " 'a u t',\n",
       " 'p h o',\n",
       " 'h i n',\n",
       " 'a l o',\n",
       " 'e d e',\n",
       " 'd 8 a',\n",
       " 's s e',\n",
       " 'a l a',\n",
       " 'o u k',\n",
       " 'p e o',\n",
       " 'o d u',\n",
       " 'l i c',\n",
       " 'a r a',\n",
       " 'o d e',\n",
       " 'l a t',\n",
       " 'e n i',\n",
       " 'e r o',\n",
       " 'e a d',\n",
       " 'r i s',\n",
       " 'o m v',\n",
       " 'c k e',\n",
       " 'n a t',\n",
       " 'c i t',\n",
       " 'h o w',\n",
       " '1 7 2',\n",
       " 't m p',\n",
       " 's p x',\n",
       " 's o r',\n",
       " 't s c',\n",
       " 'c e n',\n",
       " 'b a s',\n",
       " 'p a t',\n",
       " 'n s t',\n",
       " 'l e t',\n",
       " 'i d 3',\n",
       " 's h i',\n",
       " 'n t o',\n",
       " 'r o u',\n",
       " 's b a',\n",
       " 'o w n',\n",
       " 'u c t',\n",
       " 'r v i',\n",
       " 's n e',\n",
       " 'o i n',\n",
       " 'n h t',\n",
       " 'd e t',\n",
       " 'o c k',\n",
       " 'o g s',\n",
       " 'i a c',\n",
       " 'o l l',\n",
       " 't e l',\n",
       " 'r m a',\n",
       " 'a l s',\n",
       " 'a c c',\n",
       " 'b a t',\n",
       " '1 0 0',\n",
       " 'n g s',\n",
       " 'a r k',\n",
       " 'r a d',\n",
       " 'l o a',\n",
       " 'o m u',\n",
       " 'o m f',\n",
       " 'i d 4',\n",
       " 'o f f',\n",
       " 'u b e',\n",
       " 's o u',\n",
       " 'e r r',\n",
       " 'e b s',\n",
       " 'i m e',\n",
       " 'o r r',\n",
       " 'g e r',\n",
       " 'o v e',\n",
       " 'o n i',\n",
       " 'a r y',\n",
       " 'e s o',\n",
       " 'a t o',\n",
       " 'o t e',\n",
       " 's w e',\n",
       " 'n g c',\n",
       " 'u t u',\n",
       " 't i m',\n",
       " 'e n g',\n",
       " 'c i a',\n",
       " 'b c o',\n",
       " 'o c a',\n",
       " 'e r y',\n",
       " 't a n',\n",
       " 'a u l',\n",
       " 'f i n',\n",
       " 'i c h',\n",
       " 'e a m',\n",
       " 'b r a',\n",
       " 'l l a',\n",
       " 't a s',\n",
       " 'e l a',\n",
       " 'o l i',\n",
       " 'g a l',\n",
       " 'r e f',\n",
       " 'a d a',\n",
       " 'c e c',\n",
       " 'a n n',\n",
       " 'o p i',\n",
       " 'o n g',\n",
       " 'p o t',\n",
       " 'i a l',\n",
       " 'h o p',\n",
       " 'o v i',\n",
       " 'g r o',\n",
       " 'a m i',\n",
       " 'l a c',\n",
       " 'l e g',\n",
       " 'm w a',\n",
       " 'e x e',\n",
       " '1 2 3',\n",
       " 't d e',\n",
       " 'd o c',\n",
       " 'o r a',\n",
       " 'u n d',\n",
       " 't h o',\n",
       " '1 1 5',\n",
       " 'a s h',\n",
       " 'o n d',\n",
       " 'a t h',\n",
       " 's a s',\n",
       " 'c t s',\n",
       " 's o f',\n",
       " 'r e v',\n",
       " 'r y c',\n",
       " 'o r s',\n",
       " 'a v e',\n",
       " 'e t s',\n",
       " 'c a s',\n",
       " 'r i a',\n",
       " 'b r o',\n",
       " 's h a',\n",
       " 'n e c',\n",
       " 'c u r',\n",
       " 'e p a',\n",
       " 't t l',\n",
       " 'k e t',\n",
       " 'o t c',\n",
       " 't u b',\n",
       " 'n d o',\n",
       " 'e t e',\n",
       " 'e f a',\n",
       " 'o u s',\n",
       " 'a s c',\n",
       " 'i e n',\n",
       " 'u t h',\n",
       " 'n t h',\n",
       " 'd e o',\n",
       " 'n i c',\n",
       " 'r n e',\n",
       " 'n s a',\n",
       " 'i s h',\n",
       " '2 1 1',\n",
       " 'r a i',\n",
       " 'e 3 8',\n",
       " 'm a s',\n",
       " 'b s c',\n",
       " 's 2 0',\n",
       " 'm e t',\n",
       " 's e s',\n",
       " 'm v i',\n",
       " 'i s c',\n",
       " 'o k c',\n",
       " 'c g i',\n",
       " 's e n',\n",
       " 'n t c',\n",
       " 'r a l',\n",
       " 'n i t',\n",
       " 'r t h',\n",
       " 'm p e',\n",
       " 'h e a',\n",
       " 'l a r',\n",
       " 's m a',\n",
       " 'a n k',\n",
       " 'i l i',\n",
       " 'a s k',\n",
       " 'c r e',\n",
       " '1 0 1',\n",
       " 'm p r',\n",
       " 'e a n',\n",
       " 'o o t',\n",
       " 'h e n',\n",
       " 's i d',\n",
       " 'w e r',\n",
       " 'i e r',\n",
       " 't i s',\n",
       " 'i t s',\n",
       " 'n t p',\n",
       " 'e m o',\n",
       " 'd e f',\n",
       " 'v i c',\n",
       " 'a t a',\n",
       " 'm i c',\n",
       " 'p e n',\n",
       " 'r u m',\n",
       " 'u e s',\n",
       " 'r t a',\n",
       " 'g s p',\n",
       " 'p a n',\n",
       " 'd e c',\n",
       " 'r o s',\n",
       " 'l i t',\n",
       " 'p u b',\n",
       " 'a c a',\n",
       " 'r g e',\n",
       " 's a l',\n",
       " 's i o',\n",
       " 'p i n',\n",
       " 'e n a',\n",
       " 'm a u',\n",
       " 't b a',\n",
       " 'p a c',\n",
       " 'm i l',\n",
       " 'm m a',\n",
       " 'a l t',\n",
       " '0 0 9',\n",
       " 'k e r',\n",
       " '1 8 2',\n",
       " 'r e l',\n",
       " 'p o s',\n",
       " 'n l o',\n",
       " 'a h o',\n",
       " 'o l o',\n",
       " 'i d 6',\n",
       " 'i s p',\n",
       " 'r s o',\n",
       " 'r o c',\n",
       " 'e r m',\n",
       " 'c h v',\n",
       " 'i g n',\n",
       " 'e c u',\n",
       " 'e t r',\n",
       " 'e t c',\n",
       " 'e b e',\n",
       " 'm o r',\n",
       " 't o s',\n",
       " 'l o c',\n",
       " 'r d e',\n",
       " 'a b o',\n",
       " 'y e r',\n",
       " 'n o r',\n",
       " 'k e y',\n",
       " 't h a',\n",
       " 'n a d',\n",
       " 'e o r',\n",
       " 'n s c',\n",
       " 'a v i',\n",
       " 'r c e',\n",
       " 'g l e',\n",
       " 'o m g',\n",
       " 'l i f',\n",
       " 't o m',\n",
       " 's s o',\n",
       " 'o m h',\n",
       " 'a y o',\n",
       " 'e e n',\n",
       " 't p a',\n",
       " '0 1 5',\n",
       " 'u i n',\n",
       " '2 2 1',\n",
       " '1 1 2',\n",
       " 'm a c',\n",
       " 'u r i',\n",
       " 't p r',\n",
       " 'r a p',\n",
       " 's s t',\n",
       " 'o f t',\n",
       " 'u r c',\n",
       " 't i e',\n",
       " 'r e p',\n",
       " 'd i o',\n",
       " 't n e',\n",
       " 's i g',\n",
       " 'h i s',\n",
       " '2 2 2',\n",
       " 'g a m',\n",
       " 'v e l',\n",
       " 'r s t',\n",
       " 'r i d',\n",
       " 'c m d',\n",
       " 'd e l',\n",
       " 'o r u',\n",
       " 'o n o',\n",
       " 'd a y',\n",
       " 'd r e',\n",
       " 'm a l',\n",
       " 'd o w',\n",
       " 'r u n',\n",
       " 'e r p',\n",
       " 'm o d',\n",
       " 'o r n',\n",
       " 'i c i',\n",
       " 'i l m',\n",
       " 'h c o',\n",
       " 'd e i',\n",
       " 'r e m',\n",
       " '1 2 1',\n",
       " 'o b i',\n",
       " 'o m j',\n",
       " 'p l o',\n",
       " 'e t h',\n",
       " 'b o u',\n",
       " 'r a m',\n",
       " 'v e s',\n",
       " 'n e a',\n",
       " 'z o n',\n",
       " 'u s b',\n",
       " 't l i',\n",
       " 'n a r',\n",
       " 's p e',\n",
       " 'i n h',\n",
       " '2 1 2',\n",
       " 'b i l',\n",
       " 's t c',\n",
       " 'a s i',\n",
       " 'o o d',\n",
       " 'm n e',\n",
       " 'r n a',\n",
       " 'g r e',\n",
       " 'r o m',\n",
       " 'i t h',\n",
       " 'm o v',\n",
       " 'd a n',\n",
       " 'f i r',\n",
       " 'd 0 b',\n",
       " 'a d s',\n",
       " 'c e l',\n",
       " 's s c',\n",
       " 'b l e',\n",
       " 'i c t',\n",
       " 'e l o',\n",
       " 't a g',\n",
       " 'c l a',\n",
       " 'i d a',\n",
       " 'e x t',\n",
       " 'o u p',\n",
       " '0 1 2',\n",
       " 'i s s',\n",
       " 'a r s',\n",
       " 'w i t',\n",
       " 's o l',\n",
       " 'n k e',\n",
       " 'u t e',\n",
       " 'm t o',\n",
       " 's r e',\n",
       " 'r d s',\n",
       " 'r y i',\n",
       " 'l t o',\n",
       " 'a i r',\n",
       " 'c a c',\n",
       " 'h o c',\n",
       " 't s t',\n",
       " 'o r l',\n",
       " 't i v',\n",
       " 'o l s',\n",
       " 'r a s',\n",
       " 'l e b',\n",
       " 'l o r',\n",
       " 'i f e',\n",
       " 'm m e',\n",
       " 'o r c',\n",
       " 'h e l',\n",
       " 'n c h',\n",
       " 'o g r',\n",
       " 'i b i',\n",
       " '1 2 0',\n",
       " 'a r m',\n",
       " 'r s e',\n",
       " 'l i a',\n",
       " 'e g e',\n",
       " 'g o o',\n",
       " 's d e',\n",
       " 'n d s',\n",
       " '0 0 1',\n",
       " 'a n o',\n",
       " '1 7 1',\n",
       " 'a y e',\n",
       " 'l l o',\n",
       " 'e r g',\n",
       " 'a r r',\n",
       " 't h i',\n",
       " 's c i',\n",
       " 'l s c',\n",
       " 'n a n',\n",
       " 'd e a',\n",
       " 'n g l',\n",
       " 'e e d',\n",
       " 'a k e',\n",
       " 'e t o',\n",
       " 'k a n',\n",
       " 's s a',\n",
       " 'f i c',\n",
       " 's a r',\n",
       " 'b l i',\n",
       " 'f f e',\n",
       " 'a l b',\n",
       " 'm c a',\n",
       " 'n g a',\n",
       " 'l i v',\n",
       " 's p r',\n",
       " 'e r b',\n",
       " 'u r a',\n",
       " 's l a',\n",
       " '1 2 2',\n",
       " 'r o f',\n",
       " 'v i n',\n",
       " 'e d a',\n",
       " 'e n n',\n",
       " 'v i s',\n",
       " 'k e d',\n",
       " 't h t',\n",
       " 'i e t',\n",
       " 'c a m',\n",
       " 'e r h',\n",
       " 's t h',\n",
       " 't o o',\n",
       " 's h e',\n",
       " 'l d e',\n",
       " 'o s e',\n",
       " 'l i m',\n",
       " '4 2 2',\n",
       " 'w i n',\n",
       " 'i s e',\n",
       " 'r e t',\n",
       " 'i l s',\n",
       " 'r o w',\n",
       " 'l l s',\n",
       " 'r o p',\n",
       " 'o w s',\n",
       " 'l i e',\n",
       " 'p i d',\n",
       " 'h o r',\n",
       " 'a p e',\n",
       " 's t u',\n",
       " 'd o n',\n",
       " 'b a r',\n",
       " 'i d 7',\n",
       " 'v a l',\n",
       " '2 2 3',\n",
       " 'y i d',\n",
       " 'i l t',\n",
       " 'r t e',\n",
       " 'o n f',\n",
       " 'n g o',\n",
       " 'i t t',\n",
       " 'y a h',\n",
       " '8 2 1',\n",
       " 't m a',\n",
       " 't r u',\n",
       " 'h e s',\n",
       " 'u t o',\n",
       " 'c a p',\n",
       " 'g e t',\n",
       " 'f a u',\n",
       " '1 5 0',\n",
       " 'n s u',\n",
       " 'o r k',\n",
       " 'f a m',\n",
       " '1 2 5',\n",
       " 'i r t',\n",
       " 'n c a',\n",
       " '7 2 1',\n",
       " 'n t l',\n",
       " 'o l u',\n",
       " 'm s e',\n",
       " 'z i a',\n",
       " 'n r e',\n",
       " 'm b e',\n",
       " 'e w c',\n",
       " 't f o',\n",
       " 'o t t',\n",
       " 'm r e',\n",
       " 'u n e',\n",
       " 'd e m',\n",
       " 'n g t',\n",
       " 'm s t',\n",
       " 'a p h',\n",
       " 'l e d',\n",
       " 'o n n',\n",
       " 'n s i',\n",
       " 'e p o',\n",
       " '1 0 2',\n",
       " '1 1 3',\n",
       " 'g e o',\n",
       " 'n i a',\n",
       " 'g a r',\n",
       " 'e r d',\n",
       " '1 5 5',\n",
       " 'r s i',\n",
       " 'o o c',\n",
       " 'c l i',\n",
       " 'c l u',\n",
       " 'o l d',\n",
       " 'e 2 0',\n",
       " 'h e m',\n",
       " 'e w e',\n",
       " 'c e r',\n",
       " 'r e g',\n",
       " 'a r l',\n",
       " 'n s e',\n",
       " 'm i s',\n",
       " 'e a c',\n",
       " 'n d r',\n",
       " 'l o n',\n",
       " 'u d e',\n",
       " '1 9 1',\n",
       " 'o m q',\n",
       " 'f o o',\n",
       " 'm p u',\n",
       " 'g i b',\n",
       " 'e b r',\n",
       " 'c i n',\n",
       " 'r o o',\n",
       " 'm a z',\n",
       " 'i s i',\n",
       " 'a b a',\n",
       " 'u s a',\n",
       " 'e m p',\n",
       " 'm p h',\n",
       " 't o c',\n",
       " 'e l s',\n",
       " 'e r f',\n",
       " 'h a m',\n",
       " 'u t i',\n",
       " 't t o',\n",
       " 's o c',\n",
       " '2 5 2',\n",
       " 'i c c',\n",
       " 'm d i',\n",
       " 'r g a',\n",
       " '8 5 d',\n",
       " 'r i v',\n",
       " 'n e l',\n",
       " 'h o s',\n",
       " 'm o t',\n",
       " 'a g a',\n",
       " 'i n o',\n",
       " 'o f i',\n",
       " 'f r a',\n",
       " 'h i l',\n",
       " '3 d 3',\n",
       " 'u r n',\n",
       " 'e x a',\n",
       " 'r e i',\n",
       " 's u p',\n",
       " 'y r i',\n",
       " 'n i e',\n",
       " 'd l o',\n",
       " '2 1 8',\n",
       " 'c h c',\n",
       " 's t s',\n",
       " 'd i c',\n",
       " '5 d 8',\n",
       " 'n e d',\n",
       " 'a s o',\n",
       " 'e f o',\n",
       " 'p o l',\n",
       " 'e p h',\n",
       " 'i n n',\n",
       " 'b i o',\n",
       " 'r a r',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorize_Layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad60da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ed53eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a65900f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       "array([ 800,  213,  816, 2015, 3585,  569, 1343, 1018,   41,  217,  126,\n",
       "        102,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorize_Layer(\"hellobrother.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97135396",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c7f1cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 64)            320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 64)            33024     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 390,273\n",
      "Trainable params: 390,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "281c1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cdfb030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9631/9631 [==============================] - 336s 35ms/step - loss: 0.2337 - accuracy: 0.8974\n",
      "Epoch 2/5\n",
      "9631/9631 [==============================] - 350s 36ms/step - loss: 0.1738 - accuracy: 0.9272\n",
      "Epoch 3/5\n",
      "9631/9631 [==============================] - 350s 36ms/step - loss: 0.1478 - accuracy: 0.9390\n",
      "Epoch 4/5\n",
      "9631/9631 [==============================] - 351s 36ms/step - loss: 0.1301 - accuracy: 0.9466\n",
      "Epoch 5/5\n",
      "9631/9631 [==============================] - 361s 37ms/step - loss: 0.1160 - accuracy: 0.9530\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d1727e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2408/2408 [==============================] - 34s 14ms/step\n",
      "Confusion Matrix:\n",
      "[[63729  6271]\n",
      " [ 3757 80326]]\n",
      "Accuracy: 0.9349181934411973\n",
      "Precision: 0.935191404475249\n",
      "Recall: 0.9349181934411973\n",
      "F1 Score: 0.9348024721272894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[63729,  6271],\n",
       "        [ 3757, 80326]]),\n",
       " 0.9349181934411973,\n",
       " 0.935191404475249,\n",
       " 0.9349181934411973,\n",
       " 0.9348024721272894]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bd58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de021990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d01a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import re\n",
    "# from nltk.util import ngrams\n",
    "# import itertools\n",
    "# N_for_NGram = 3\n",
    "# Sequence_length = 200\n",
    "# Batch_Size = 64\n",
    "# epochs = 5\n",
    "# n_classes = 2\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "# url_dataframe['url'] = url_dataframe['url'].str.replace(\"https://\",\"\")\n",
    "# url_dataframe['url'] = url_dataframe['url'].str.replace(\"http://\",\"\")\n",
    "# url_dataframe['url'] = url_dataframe['url'].str.replace(\" \",\"\")\n",
    "# url_dataframe['url'] = url_dataframe['url'].str.replace(\"www\",\"\")\n",
    "# url_dataframe['url'] = url_dataframe['url'].str.strip()\n",
    "# print(url_dataframe)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "# train_ds = train_ds.batch(Batch_Size)\n",
    "# test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "# Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "#                                                    split=\"character\",\n",
    "#                                                    ngrams=(N_for_NGram,),\n",
    "#                                                    output_mode='int',\n",
    "#                                                    output_sequence_length=Sequence_length)\n",
    "\n",
    "# # Make a text-only dataset (without labels), then call adapt\n",
    "# train_text = train_ds.map(lambda x, y: x)\n",
    "# Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "# max_tokens = len(Vectorize_Layer.get_vocabulary())\n",
    "# print(\"max_tokens\" = max_tokens)\n",
    "\n",
    "# print(Vectorize_Layer.get_vocabulary())\n",
    "\n",
    "# def vectorize_text(text, label):\n",
    "#   text = tf.expand_dims(text, -1)\n",
    "#   return Vectorize_Layer(text), label\n",
    "\n",
    "# train_ds = train_ds.map(vectorize_text)\n",
    "# test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "#   tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "#   tf.keras.layers.LSTM(64,) ,\n",
    "#   tf.keras.layers.Dense(64, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.2),\n",
    "#   tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     epochs=epochs)\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Define the evaluation function\n",
    "# def evaluate_model(model, test_dataset):\n",
    "#     y_true = []\n",
    "#     for x, y in test_dataset:\n",
    "#         for yi in y:\n",
    "#             y_true.append(yi.numpy())\n",
    "    \n",
    "#     y_true = np.array(y_true)\n",
    "#     y_true = y_true.flatten()\n",
    "#     y_pred = model.predict(test_dataset)\n",
    "#     y_pred = y_pred.flatten()\n",
    "#     y_pred = [round(i) for i in y_pred]\n",
    "#     print(y_true)\n",
    "#     print(y_pred)\n",
    "#     # Confusion matrix\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(cm)\n",
    "\n",
    "#     # Accuracy\n",
    "#     acc = accuracy_score(y_true, y_pred)\n",
    "#     print(\"Accuracy:\", acc)\n",
    "\n",
    "#     # Precision\n",
    "#     prec = precision_score(y_true, y_pred, average='weighted')\n",
    "#     print(\"Precision:\", prec)\n",
    "\n",
    "#     # Recall\n",
    "#     rec = recall_score(y_true, y_pred, average='weighted')\n",
    "#     print(\"Recall:\", rec)\n",
    "\n",
    "#     # F1 Score\n",
    "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "#     print(\"F1 Score:\", f1)\n",
    "    \n",
    "#     return [cm,acc,prec,rec,f1]\n",
    "# # Evaluate the model\n",
    "# result = evaluate_model(model, test_ds)\n",
    "\n",
    "# import pickle\n",
    "# with open(\"model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n",
    "    \n",
    "# with open(\"result.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
