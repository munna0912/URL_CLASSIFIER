{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5859db",
   "metadata": {},
   "source": [
    "# Implementation of N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209957f",
   "metadata": {},
   "source": [
    "## 2-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d01a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 00:56:48.964391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 00:56:49.080740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:56:49.080767: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 00:56:49.105188: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-04 00:56:49.630240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:56:49.630304: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:56:49.630309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-04 00:56:50.411999: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-04 00:56:50.412028: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-04 00:56:50.412045: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (munna0912-VivoBook-ASUSLaptop-X421IAY-M413IA): /proc/driver/nvidia/version does not exist\n",
      "2023-03-04 00:56:50.412277: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200, 64)           33024     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 390,273\n",
      "Trainable params: 390,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "9631/9631 [==============================] - 1265s 131ms/step - loss: 0.6242 - accuracy: 0.6117\n",
      "Epoch 2/10\n",
      "9631/9631 [==============================] - 1286s 133ms/step - loss: 0.2353 - accuracy: 0.8969\n",
      "Epoch 3/10\n",
      "9631/9631 [==============================] - 1286s 134ms/step - loss: 0.1821 - accuracy: 0.9235\n",
      "Epoch 4/10\n",
      "9631/9631 [==============================] - 1283s 133ms/step - loss: 0.1566 - accuracy: 0.9353\n",
      "Epoch 5/10\n",
      "9631/9631 [==============================] - 1286s 134ms/step - loss: 0.1386 - accuracy: 0.9430\n",
      "Epoch 6/10\n",
      "9631/9631 [==============================] - 1288s 134ms/step - loss: 0.1258 - accuracy: 0.9485\n",
      "Epoch 7/10\n",
      "9631/9631 [==============================] - 1294s 134ms/step - loss: 0.1137 - accuracy: 0.9539\n",
      "Epoch 8/10\n",
      "9631/9631 [==============================] - 1288s 134ms/step - loss: 0.1040 - accuracy: 0.9585\n",
      "Epoch 9/10\n",
      "9631/9631 [==============================] - 1290s 134ms/step - loss: 0.0952 - accuracy: 0.9622\n",
      "Epoch 10/10\n",
      "9631/9631 [==============================] - 1292s 134ms/step - loss: 0.0876 - accuracy: 0.9657\n",
      "2408/2408 [==============================] - 118s 49ms/step\n",
      "Confusion Matrix:\n",
      "[[63737  5810]\n",
      " [ 3901 80635]]\n",
      "Accuracy: 0.9369755261774498\n",
      "Precision: 0.9370936678463961\n",
      "Recall: 0.9369755261774498\n",
      "F1 Score: 0.9368888507519502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d6004f0e-44c6-41bb-8192-5836045e03cd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d6004f0e-44c6-41bb-8192-5836045e03cd/assets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 5000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_5k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_5k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0709884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 64)           640000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 200, 64)           33024     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 710,273\n",
      "Trainable params: 710,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "9631/9631 [==============================] - 1307s 135ms/step - loss: 0.6606 - accuracy: 0.5989\n",
      "Epoch 2/10\n",
      "9631/9631 [==============================] - 1300s 135ms/step - loss: 0.4595 - accuracy: 0.7588\n",
      "Epoch 3/10\n",
      "9631/9631 [==============================] - 1301s 135ms/step - loss: 0.1879 - accuracy: 0.9215\n",
      "Epoch 4/10\n",
      "9631/9631 [==============================] - 1307s 136ms/step - loss: 0.1498 - accuracy: 0.9383\n",
      "Epoch 5/10\n",
      "9631/9631 [==============================] - 1303s 135ms/step - loss: 0.1264 - accuracy: 0.9485\n",
      "Epoch 6/10\n",
      "9631/9631 [==============================] - 1311s 136ms/step - loss: 0.1094 - accuracy: 0.9560\n",
      "Epoch 7/10\n",
      "9631/9631 [==============================] - 1364s 142ms/step - loss: 0.0951 - accuracy: 0.9622\n",
      "Epoch 8/10\n",
      "9631/9631 [==============================] - 1306s 136ms/step - loss: 0.0831 - accuracy: 0.9674\n",
      "Epoch 9/10\n",
      "9631/9631 [==============================] - 1306s 136ms/step - loss: 0.0726 - accuracy: 0.9719\n",
      "Epoch 10/10\n",
      "9631/9631 [==============================] - 1308s 136ms/step - loss: 0.0635 - accuracy: 0.9756\n",
      "2408/2408 [==============================] - 115s 48ms/step\n",
      "Confusion Matrix:\n",
      "[[64520  4908]\n",
      " [ 4581 80074]]\n",
      "Accuracy: 0.9384163080936898\n",
      "Precision: 0.9383981824038453\n",
      "Recall: 0.9384163080936898\n",
      "F1 Score: 0.9384029791944256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://23c3bf3e-892f-4354-bc0d-873837bde72d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://23c3bf3e-892f-4354-bc0d-873837bde72d/assets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 10000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_10k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_10k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de61261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 200, 64)           960000    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 200, 64)           33024     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,030,273\n",
      "Trainable params: 1,030,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "9631/9631 [==============================] - 1349s 140ms/step - loss: 0.6687 - accuracy: 0.5791\n",
      "Epoch 2/10\n",
      "9631/9631 [==============================] - 1319s 137ms/step - loss: 0.2294 - accuracy: 0.9013\n",
      "Epoch 3/10\n",
      "9631/9631 [==============================] - 1404s 146ms/step - loss: 0.1521 - accuracy: 0.9371\n",
      "Epoch 4/10\n",
      "9631/9631 [==============================] - 1315s 137ms/step - loss: 0.1201 - accuracy: 0.9512\n",
      "Epoch 5/10\n",
      "9631/9631 [==============================] - 1331s 138ms/step - loss: 0.0983 - accuracy: 0.9609\n",
      "Epoch 6/10\n",
      "9631/9631 [==============================] - 1393s 145ms/step - loss: 0.0819 - accuracy: 0.9680\n",
      "Epoch 7/10\n",
      "9631/9631 [==============================] - 1333s 138ms/step - loss: 0.0684 - accuracy: 0.9738\n",
      "Epoch 8/10\n",
      "9631/9631 [==============================] - 1376s 143ms/step - loss: 0.0574 - accuracy: 0.9784\n",
      "Epoch 9/10\n",
      "9631/9631 [==============================] - 1400s 145ms/step - loss: 0.0484 - accuracy: 0.9822\n",
      "Epoch 10/10\n",
      "9631/9631 [==============================] - 1407s 146ms/step - loss: 0.0410 - accuracy: 0.9850\n",
      "2408/2408 [==============================] - 123s 51ms/step\n",
      "Confusion Matrix:\n",
      "[[65508  4173]\n",
      " [ 5612 78790]]\n",
      "Accuracy: 0.9364952655387032\n",
      "Precision: 0.9367624518099839\n",
      "Recall: 0.9364952655387032\n",
      "F1 Score: 0.9365467717960131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://807d443d-14c5-4203-87ca-fef92b80eef3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://807d443d-14c5-4203-87ca-fef92b80eef3/assets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 15000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_15k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_15k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5658aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 200, 64)           1280000   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 200, 64)           33024     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,350,273\n",
      "Trainable params: 1,350,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "9631/9631 [==============================] - 1426s 148ms/step - loss: 0.6705 - accuracy: 0.5786\n",
      "Epoch 2/10\n",
      "9631/9631 [==============================] - 1359s 141ms/step - loss: 0.3309 - accuracy: 0.8366\n",
      "Epoch 3/10\n",
      "9631/9631 [==============================] - 1458s 151ms/step - loss: 0.1788 - accuracy: 0.9251\n",
      "Epoch 4/10\n",
      "9631/9631 [==============================] - 1442s 150ms/step - loss: 0.1416 - accuracy: 0.9425\n",
      "Epoch 5/10\n",
      "9631/9631 [==============================] - 1363s 142ms/step - loss: 0.1168 - accuracy: 0.9532\n",
      "Epoch 6/10\n",
      "9631/9631 [==============================] - 1357s 141ms/step - loss: 0.0951 - accuracy: 0.9628\n",
      "Epoch 7/10\n",
      "9631/9631 [==============================] - 1411s 146ms/step - loss: 0.0785 - accuracy: 0.9700\n",
      "Epoch 8/10\n",
      "9631/9631 [==============================] - 1443s 150ms/step - loss: 0.0657 - accuracy: 0.9756\n",
      "Epoch 9/10\n",
      "9631/9631 [==============================] - 1381s 143ms/step - loss: 0.0552 - accuracy: 0.9800\n",
      "Epoch 10/10\n",
      "9631/9631 [==============================] - 1319s 137ms/step - loss: 0.0469 - accuracy: 0.9834\n",
      "2408/2408 [==============================] - 113s 46ms/step\n",
      "Confusion Matrix:\n",
      "[[64359  5182]\n",
      " [ 4579 79963]]\n",
      "Accuracy: 0.9366510257458642\n",
      "Precision: 0.9366292624484391\n",
      "Recall: 0.9366510257458642\n",
      "F1 Score: 0.9366256592574238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://10451035-61d9-4ec9-a8fc-3851c3a19b8b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://10451035-61d9-4ec9-a8fc-3851c3a19b8b/assets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens =20000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_20k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_20k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f1301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 200, 64)           1600000   \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 200, 64)           33024     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,670,273\n",
      "Trainable params: 1,670,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "9631/9631 [==============================] - 1332s 138ms/step - loss: 0.6353 - accuracy: 0.6075\n",
      "Epoch 2/10\n",
      "9631/9631 [==============================] - 1327s 138ms/step - loss: 0.2174 - accuracy: 0.9053\n",
      "Epoch 3/10\n",
      "9631/9631 [==============================] - 1329s 138ms/step - loss: 0.1470 - accuracy: 0.9391\n",
      "Epoch 4/10\n",
      "9631/9631 [==============================] - 1344s 140ms/step - loss: 0.1152 - accuracy: 0.9532\n",
      "Epoch 5/10\n",
      "9631/9631 [==============================] - 1346s 140ms/step - loss: 0.0940 - accuracy: 0.9627\n",
      "Epoch 6/10\n",
      "9631/9631 [==============================] - 1346s 140ms/step - loss: 0.0764 - accuracy: 0.9703\n",
      "Epoch 7/10\n",
      "9631/9631 [==============================] - 1348s 140ms/step - loss: 0.0622 - accuracy: 0.9763\n",
      "Epoch 8/10\n",
      "9631/9631 [==============================] - 1349s 140ms/step - loss: 0.0519 - accuracy: 0.9806\n",
      "Epoch 9/10\n",
      "9631/9631 [==============================] - 1350s 140ms/step - loss: 0.0423 - accuracy: 0.9843\n",
      "Epoch 10/10\n",
      "9631/9631 [==============================] - 1391s 144ms/step - loss: 0.0358 - accuracy: 0.9871\n",
      "2408/2408 [==============================] - 120s 50ms/step\n",
      "Confusion Matrix:\n",
      "[[63686  5786]\n",
      " [ 3441 81170]]\n",
      "Accuracy: 0.9401166903551982\n",
      "Precision: 0.9403492127395188\n",
      "Recall: 0.9401166903551982\n",
      "F1 Score: 0.9400119272982336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://820649b6-0cc1-4152-93f2-7c55e3d9a9a0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://820649b6-0cc1-4152-93f2-7c55e3d9a9a0/assets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 25000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_25k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_25k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13684df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 200, 64)           1920000   \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 200, 64)           33024     \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,990,273\n",
      "Trainable params: 1,990,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "9631/9631 [==============================] - 1379s 143ms/step - loss: 0.6881 - accuracy: 0.5513\n",
      "Epoch 2/10\n",
      "9631/9631 [==============================] - 1368s 142ms/step - loss: 0.5051 - accuracy: 0.6959\n",
      "Epoch 3/10\n",
      "9631/9631 [==============================] - 1433s 149ms/step - loss: 0.1623 - accuracy: 0.9330\n",
      "Epoch 4/10\n",
      "9631/9631 [==============================] - 1418s 147ms/step - loss: 0.1173 - accuracy: 0.9530\n",
      "Epoch 5/10\n",
      "9631/9631 [==============================] - 1423s 148ms/step - loss: 0.0896 - accuracy: 0.9649\n",
      "Epoch 6/10\n",
      "9631/9631 [==============================] - 1452s 151ms/step - loss: 0.0699 - accuracy: 0.9733\n",
      "Epoch 7/10\n",
      "1566/9631 [===>..........................] - ETA: 19:13 - loss: 0.0612 - accuracy: 0.9770"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 30000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_30k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_30k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 35000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_35k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_35k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67337508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 40000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   max_tokens = max_tokens,\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_40k_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_40k_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8abfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "N_for_NGram = 3\n",
    "Sequence_length = 200\n",
    "Batch_Size = 64\n",
    "epochs = 10\n",
    "n_classes = 2\n",
    "max_tokens = 5000\n",
    "# Define a function to preprocess the link\n",
    "def preprocess_link(link):\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    link = link.lower()\n",
    "    \n",
    "    link = link.replace(\"http://\",\"\")\n",
    "    link = link.replace(\"https://\",\"\")\n",
    "    link = link.replace(\"www.\",\"\")\n",
    "    link = link.replace(\" \",\"\")\n",
    "    link = link.strip()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    link = re.sub('[^A-Za-z0-9]+', '', link)\n",
    "    \n",
    "    return link\n",
    "from sklearn.model_selection import train_test_split\n",
    "url_dataframe = pd.read_csv(\"../dataset/df_final.csv\")\n",
    "url_dataframe['url'] = url_dataframe['url'].apply(preprocess_link)\n",
    "url_dataframe = url_dataframe.drop_duplicates(subset=['url'])\n",
    "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['url'].values, train_df['type'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df['url'].values, test_df['type'].values))\n",
    "\n",
    "train_ds = train_ds.batch(Batch_Size)\n",
    "test_ds = test_ds.batch(Batch_Size)\n",
    "\n",
    "Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
    "                                                   split=\"character\",\n",
    "                                                   ngrams=(N_for_NGram,),\n",
    "                                                   output_mode='int',\n",
    "                                                   output_sequence_length=Sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = train_ds.map(lambda x, y: x)\n",
    "Vectorize_Layer.adapt(train_text)\n",
    "\n",
    "max_tokens = len(Vectorize_Layer.get_vocabulary())\n",
    "print(max_tokens)\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return Vectorize_Layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_tokens, 64, input_length=Sequence_length),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True) ,\n",
    "  tf.keras.layers.LSTM(64,) ,\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    for x, y in test_dataset:\n",
    "        for yi in y:\n",
    "            y_true.append(yi.numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = [round(i) for i in y_pred]\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Precision:\", prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Recall:\", rec)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "    return [cm,acc,prec,rec,f1]\n",
    "# Evaluate the model\n",
    "result = evaluate_model(model, test_ds)\n",
    "\n",
    "import pickle\n",
    "with open(\"3_gram_200_all_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open(\"3_gram_200_all_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e3cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb110d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
