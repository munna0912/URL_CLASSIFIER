{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8JiFmXgwGNN"
      },
      "source": [
        "# Train and Check accuracy on 20% test data"
      ],
      "id": "k8JiFmXgwGNN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f58a00ff"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/URL_Checker\")  # this appends the parent directory to the sys.path list\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from nltk.util import ngrams\n",
        "import itertools\n",
        "from Utils import Evaluation\n",
        "from Utils import DataProcessing, FeatureCreation, Model\n",
        "N_for_NGram = 3\n",
        "Sequence_length = 50\n",
        "epochs = 20\n",
        "n_classes = 2"
      ],
      "id": "f58a00ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ad4805",
        "outputId": "fd0a0881-cd00-4406-cabd-254e5e594921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ],
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "id": "f5ad4805"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9401c2b8"
      },
      "outputs": [],
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "id": "9401c2b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eaa2e0f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/URL_Checker/dataset/df_final.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/URL_Checker/dataset/all_top_domains_merged.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/URL_Checker/dataset/df_another_validation.csv\")\n",
        "url_dataframe = pd.concat([df1,df2], axis=0)\n",
        "url_dataframe = url_dataframe.sample(frac=1).reset_index(drop=True)\n",
        "url_dataframe = url_dataframe.drop_duplicates()\n",
        "url_dataframe = url_dataframe.dropna().reset_index(drop=True)"
      ],
      "id": "9eaa2e0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ade05ed"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(url_dataframe, test_size=0.20, stratify=url_dataframe['type'])\n",
        "# train_df, val_df = train_test_split(train_df, test_size=0.20, stratify=train_df['type'])"
      ],
      "id": "5ade05ed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68bc2e77",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "Batch_Size = 16 * strategy.num_replicas_in_sync\n",
        "# Batch_Size = 128\n",
        "max_tokens = 10000\n",
        "with strategy.scope():\n",
        "  train_ds = DataProcessing.process_train_data(train_df,Batch_Size)\n",
        "  Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
        "                                                    split=\"character\",\n",
        "                                                    ngrams=(N_for_NGram,),\n",
        "                                                    output_mode='int',\n",
        "                                                    max_tokens = max_tokens,\n",
        "                                                    output_sequence_length=Sequence_length)\n",
        "  # Make a text-only dataset (without labels), then call adapt\n",
        "  train_text = train_ds.map(lambda x,z: x[0])\n",
        "  Vectorize_Layer.adapt(train_text)\n",
        "  train_ds = train_ds.map(lambda x, z: DataProcessing.vectorize_text(x[0],x[1],z,Vectorize_Layer))\n",
        "  train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "id": "68bc2e77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff7e49bd"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  val_ds = DataProcessing.process_data(val_df,Batch_Size,Vectorize_Layer)\n",
        "  test_ds = DataProcessing.process_data(test_df,Batch_Size,Vectorize_Layer)"
      ],
      "id": "ff7e49bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd8a8373",
        "outputId": "bed313e1-01ef-4711-a4ed-cd6cef672402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 50, 16)       160000      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 18)]         0           []                               \n",
            "                                                                                                  \n",
            " gru (GRU)                      (None, 16)           1632        ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           304         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 16)           0           ['gru[0][0]']                    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 16)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32)           0           ['dropout[0][0]',                \n",
            "                                                                  'dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 16)           528         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 16)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            17          ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 162,481\n",
            "Trainable params: 162,481\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "  model = Model.create_model(Sequence_length, max_tokens, 18)\n",
        "  print(model.summary())\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "id": "cd8a8373"
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    batch_size=Batch_Size,\n",
        "    validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vteRUJSK-iff",
        "outputId": "6a02264c-e986-4d65-e682-6287dc3d1923"
      },
      "id": "vteRUJSK-iff",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4846/4846 [==============================] - 144s 28ms/step - loss: 0.1991 - accuracy: 0.9298 - val_loss: 0.0744 - val_accuracy: 0.9750\n",
            "Epoch 2/20\n",
            "4846/4846 [==============================] - 123s 25ms/step - loss: 0.0809 - accuracy: 0.9732 - val_loss: 0.0627 - val_accuracy: 0.9783\n",
            "Epoch 3/20\n",
            "4846/4846 [==============================] - 122s 25ms/step - loss: 0.0689 - accuracy: 0.9771 - val_loss: 0.0592 - val_accuracy: 0.9800\n",
            "Epoch 4/20\n",
            "4846/4846 [==============================] - 121s 25ms/step - loss: 0.0621 - accuracy: 0.9797 - val_loss: 0.0536 - val_accuracy: 0.9820\n",
            "Epoch 5/20\n",
            "4846/4846 [==============================] - 121s 25ms/step - loss: 0.0567 - accuracy: 0.9813 - val_loss: 0.0552 - val_accuracy: 0.9817\n",
            "Epoch 6/20\n",
            "4846/4846 [==============================] - 120s 25ms/step - loss: 0.0525 - accuracy: 0.9830 - val_loss: 0.0499 - val_accuracy: 0.9832\n",
            "Epoch 7/20\n",
            "4846/4846 [==============================] - 119s 25ms/step - loss: 0.0486 - accuracy: 0.9841 - val_loss: 0.0522 - val_accuracy: 0.9829\n",
            "Epoch 8/20\n",
            "4846/4846 [==============================] - 120s 25ms/step - loss: 0.0461 - accuracy: 0.9849 - val_loss: 0.0497 - val_accuracy: 0.9837\n",
            "Epoch 9/20\n",
            "4846/4846 [==============================] - 119s 25ms/step - loss: 0.0433 - accuracy: 0.9856 - val_loss: 0.0488 - val_accuracy: 0.9833\n",
            "Epoch 10/20\n",
            "4846/4846 [==============================] - 118s 24ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.0479 - val_accuracy: 0.9844\n",
            "Epoch 11/20\n",
            "4846/4846 [==============================] - 122s 25ms/step - loss: 0.0392 - accuracy: 0.9870 - val_loss: 0.0493 - val_accuracy: 0.9842\n",
            "Epoch 12/20\n",
            "4846/4846 [==============================] - 121s 25ms/step - loss: 0.0373 - accuracy: 0.9876 - val_loss: 0.0510 - val_accuracy: 0.9828\n",
            "Epoch 13/20\n",
            "4846/4846 [==============================] - 122s 25ms/step - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.0509 - val_accuracy: 0.9840\n",
            "Epoch 14/20\n",
            "4846/4846 [==============================] - 120s 25ms/step - loss: 0.0347 - accuracy: 0.9883 - val_loss: 0.0503 - val_accuracy: 0.9844\n",
            "Epoch 15/20\n",
            "4846/4846 [==============================] - 120s 25ms/step - loss: 0.0338 - accuracy: 0.9887 - val_loss: 0.0521 - val_accuracy: 0.9836\n",
            "Epoch 16/20\n",
            "4846/4846 [==============================] - 119s 25ms/step - loss: 0.0333 - accuracy: 0.9888 - val_loss: 0.0516 - val_accuracy: 0.9837\n",
            "Epoch 17/20\n",
            "4846/4846 [==============================] - 120s 25ms/step - loss: 0.0322 - accuracy: 0.9892 - val_loss: 0.0524 - val_accuracy: 0.9839\n",
            "Epoch 18/20\n",
            "4846/4846 [==============================] - 119s 25ms/step - loss: 0.0309 - accuracy: 0.9895 - val_loss: 0.0526 - val_accuracy: 0.9836\n",
            "Epoch 19/20\n",
            "4846/4846 [==============================] - 121s 25ms/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.0541 - val_accuracy: 0.9827\n",
            "Epoch 20/20\n",
            "4846/4846 [==============================] - 120s 25ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.0565 - val_accuracy: 0.9834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = Evaluation.evaluate_model(model, test_ds, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nQdN_YB-ogv",
        "outputId": "de7e7c7e-36cf-4522-9de0-ec24cc148f52"
      },
      "id": "7nQdN_YB-ogv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function Executor.__del__ at 0x7ce86885b880>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 46, in __del__\n",
            "    self.wait()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 65, in wait\n",
            "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3901/3901 [==============================] - 64s 16ms/step\n",
            "Confusion Matrix:\n",
            "[[  8520  98103]\n",
            " [ 46160 346534]]\n",
            "Accuracy: 0.7110793343707504\n",
            "Precision: 0.6462127998444198\n",
            "Recall: 0.7110793343707504\n",
            "F1 Score: 0.6735214855648405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it represents over 98.4% accuracy on validation data i.e. 20% of total data. and 71% accuracy for another dataset, which is not belongs to our dataset (it means our model is more generalized and will perform great for unseen data.)"
      ],
      "metadata": {
        "id": "oHhBPM5SbXMc"
      },
      "id": "oHhBPM5SbXMc"
    },
    {
      "cell_type": "code",
      "source": [
        "links = [\"itunes.apple.com/gb/album/whb-bonus-track-version/id358709625\", \"http://45.231.210.144:47317/Mozi.m\", \"www.google.com\"]"
      ],
      "metadata": {
        "id": "0H9qXB5W-bxp"
      },
      "id": "0H9qXB5W-bxp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXwvbJZt9r_A",
        "outputId": "31c99ae4-ba60-4810-f0a6-2c6d48eec9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Safe', 'Malicious', 'Safe']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "Evaluation.make_prediction(model,Batch_Size, Vectorize_Layer, links, 0.5)"
      ],
      "id": "VXwvbJZt9r_A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TwKGI-5NOQQ"
      },
      "source": [
        "# Training the model on complete Dataset"
      ],
      "id": "1TwKGI-5NOQQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efGpPhVPNjxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b142dc42-740e-407a-f54d-2ff0c98b337f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 50, 16)       160000      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 18)]         0           []                               \n",
            "                                                                                                  \n",
            " gru (GRU)                      (None, 16)           1632        ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           304         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 16)           0           ['gru[0][0]']                    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 16)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32)           0           ['dropout[0][0]',                \n",
            "                                                                  'dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 16)           528         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 16)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            17          ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 162,481\n",
            "Trainable params: 162,481\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "6517/6517 [==============================] - 157s 23ms/step - loss: 0.3322 - accuracy: 0.8681\n",
            "Epoch 2/20\n",
            "6517/6517 [==============================] - 147s 23ms/step - loss: 0.1629 - accuracy: 0.9222\n",
            "Epoch 3/20\n",
            "6517/6517 [==============================] - 147s 23ms/step - loss: 0.1468 - accuracy: 0.9271\n",
            "Epoch 4/20\n",
            "6517/6517 [==============================] - 148s 23ms/step - loss: 0.1390 - accuracy: 0.9297\n",
            "Epoch 5/20\n",
            "6517/6517 [==============================] - 148s 23ms/step - loss: 0.1339 - accuracy: 0.9310\n",
            "Epoch 6/20\n",
            "6517/6517 [==============================] - 147s 23ms/step - loss: 0.1302 - accuracy: 0.9323\n",
            "Epoch 7/20\n",
            "6517/6517 [==============================] - 147s 23ms/step - loss: 0.1279 - accuracy: 0.9329\n",
            "Epoch 8/20\n",
            "6517/6517 [==============================] - 147s 23ms/step - loss: 0.1255 - accuracy: 0.9339\n",
            "Epoch 9/20\n",
            "6517/6517 [==============================] - 149s 23ms/step - loss: 0.1232 - accuracy: 0.9342\n",
            "Epoch 10/20\n",
            "6517/6517 [==============================] - 148s 23ms/step - loss: 0.1223 - accuracy: 0.9349\n",
            "Epoch 11/20\n",
            "6517/6517 [==============================] - 151s 23ms/step - loss: 0.1212 - accuracy: 0.9352\n",
            "Epoch 12/20\n",
            "6517/6517 [==============================] - 147s 23ms/step - loss: 0.1193 - accuracy: 0.9356\n",
            "Epoch 13/20\n",
            "6517/6517 [==============================] - 146s 22ms/step - loss: 0.1183 - accuracy: 0.9364\n",
            "Epoch 14/20\n",
            "6517/6517 [==============================] - 148s 23ms/step - loss: 0.1177 - accuracy: 0.9366\n",
            "Epoch 15/20\n",
            "6517/6517 [==============================] - 146s 22ms/step - loss: 0.1168 - accuracy: 0.9371\n",
            "Epoch 16/20\n",
            "6517/6517 [==============================] - 148s 23ms/step - loss: 0.1161 - accuracy: 0.9374\n",
            "Epoch 17/20\n",
            "6517/6517 [==============================] - 146s 22ms/step - loss: 0.1151 - accuracy: 0.9378\n",
            "Epoch 18/20\n",
            "6517/6517 [==============================] - 148s 23ms/step - loss: 0.1143 - accuracy: 0.9380\n",
            "Epoch 19/20\n",
            "5982/6517 [==========================>...] - ETA: 12s - loss: 0.1196 - accuracy: 0.9347"
          ]
        }
      ],
      "source": [
        "df1 = pd.read_csv(\"/content/drive/MyDrive/URL_Checker/dataset/df_final.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/URL_Checker/dataset/all_top_domains_merged.csv\")\n",
        "df3 = pd.read_csv(\"/content/drive/MyDrive/URL_Checker/dataset/df_another_validation.csv\")\n",
        "url_dataframe = pd.concat([df1,df2, df3], axis=0)\n",
        "url_dataframe = url_dataframe.sample(frac=1).reset_index(drop=True)\n",
        "url_dataframe = url_dataframe.drop_duplicates()\n",
        "url_dataframe = url_dataframe.dropna().reset_index(drop=True)\n",
        "Batch_Size = 16 * strategy.num_replicas_in_sync\n",
        "# Batch_Size = 128\n",
        "N_for_NGram = 3\n",
        "Sequence_length = 50\n",
        "epochs = 20\n",
        "n_classes = 2\n",
        "max_tokens = 10000\n",
        "with strategy.scope():\n",
        "  train_ds = DataProcessing.process_train_data(url_dataframe,Batch_Size)\n",
        "  Vectorize_Layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
        "                                                    split=\"character\",\n",
        "                                                    ngrams=(N_for_NGram,),\n",
        "                                                    output_mode='int',\n",
        "                                                    max_tokens = max_tokens,\n",
        "                                                    output_sequence_length=Sequence_length)\n",
        "\n",
        "  # Make a text-only dataset (without labels), then call adapt\n",
        "  train_text = train_ds.map(lambda x,z: x[0])\n",
        "  Vectorize_Layer.adapt(train_text)\n",
        "  train_ds = train_ds.map(lambda x, z: DataProcessing.vectorize_text(x[0],x[1],z,Vectorize_Layer))\n",
        "  train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "  model = Model.create_model(Sequence_length, max_tokens, 18)\n",
        "  print(model.summary())\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    batch_size=Batch_Size)"
      ],
      "id": "efGpPhVPNjxm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGOL_fNXOSn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcf182a-4b89-48e2-e6aa-e635f9130380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Safe', 'Malicious', 'Safe']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "links = [\"itunes.apple.com/gb/album/whb-bonus-track-version/id358709625\",\"http://45.231.210.144:47317/Mozi.m\",\"youtu.be\"]\n",
        "Evaluation.make_prediction(model,1, Vectorize_Layer, links, 0.5)"
      ],
      "id": "XGOL_fNXOSn2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O542kBtOaD4"
      },
      "source": [
        "# Save the Model and Vocabulary of Vectorization Layer"
      ],
      "id": "7O542kBtOaD4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVnInwRHAUTV"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/URL_Checker/Trained_Model/Model/Model.keras\")"
      ],
      "id": "HVnInwRHAUTV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF8isQvRD19r"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "vocab = Vectorize_Layer.get_vocabulary()\n",
        "with open(\"/content/drive/MyDrive/URL_Checker/Trained_Model/Vectorization_Layer/vocab.json\", \"w\") as f:\n",
        "    json.dump(vocab, f)"
      ],
      "id": "WF8isQvRD19r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v57vcMsU3Aw"
      },
      "outputs": [],
      "source": [
        "print(Batch_Size, Sequence_length, N_for_NGram, max_tokens)"
      ],
      "id": "1v57vcMsU3Aw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10j0p0jkOkn-"
      },
      "source": [
        "# Loading the Model and Vocabulary from Saved Model"
      ],
      "id": "10j0p0jkOkn-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cTTqzvQVFwJ"
      },
      "outputs": [],
      "source": [
        "modell = tf.keras.models.load_model(\"/content/drive/MyDrive/URL_Checker/Trained_Model/Model/Model.keras\")"
      ],
      "id": "_cTTqzvQVFwJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJz9uHjfUDZE"
      },
      "outputs": [],
      "source": [
        "# Batch_Size = 16 * strategy.num_replicas_in_sync\n",
        "Batch_Size = 128\n",
        "max_tokens = 10000\n",
        "with open(\"/content/drive/MyDrive/URL_Checker/Trained_Model/Vectorization_Layer/vocab.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "VL = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',\n",
        "                                                   split=\"character\",\n",
        "                                                   ngrams=(N_for_NGram,),\n",
        "                                                   output_mode='int',\n",
        "                                                   output_sequence_length=Sequence_length,\n",
        "                                                   vocabulary=vocab)"
      ],
      "id": "bJz9uHjfUDZE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPolGWCQVebN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8d0b42-275d-4bab-b040-9a32d7d923c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Safe', 'Malicious']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "links = [\"itunes.apple.com/gb/album/whb-bonus-track-version/id358709625\",\"http://45.231.210.144:47317/Mozi.m\"]\n",
        "Evaluation.make_prediction(modell,Batch_Size, VL, links, 0.5)"
      ],
      "id": "UPolGWCQVebN"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRQitw4XthAF"
      },
      "id": "eRQitw4XthAF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}